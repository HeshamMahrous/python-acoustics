{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hesham-Mahrous.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/HeshamMahrous/python-acoustics/blob/master/Hesham_Mahrous.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Jh_ZkVCbGsI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "This is an imperfect implementation of ResNet configured to train with the CIFAR-10 dataset.\n",
        "\n",
        "There are a few bugs and it should be able to reach a higher accuracy. See what you can do to fix it and write a few paragraphs about what you did."
      ]
    },
    {
      "metadata": {
        "id": "4iI079DBDSW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "2iVqm5Jr-078",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ff2b84f-39c5-45d0-9d3a-3d7e2f54f246"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print('TensorFlow Version:', tf.VERSION)\n",
        "print('TensorFlow GPU:', tf.test.gpu_device_name())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.9.0\n",
            "TensorFlow GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JKaCOUwrK1au",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "7gy8inTwL8lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a54ff955-0ed8-416a-fe5e-ae90ea60c28c"
      },
      "cell_type": "code",
      "source": [
        "data_name = 'cifar-10-batches-bin'\n",
        "data_dir = os.path.join('/content/.keras/datasets', data_name)\n",
        "data_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
        "tf.keras.utils.get_file(data_name, data_url, untar=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
            "136372224/170052171 [=======================>......] - ETA: 10s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170057728/170052171 [==============================] - 53s 0us/step\n",
            "170065920/170052171 [==============================] - 53s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/.keras/datasets/cifar-10-batches-bin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "H_xuw_3zKwhs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inputs"
      ]
    },
    {
      "metadata": {
        "id": "DRE_SKGgKzQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_input_fn(filenames, batch_size, num_epochs, shuffle=False, augment=False):\n",
        "    label_bytes = 1\n",
        "    image_height = 32\n",
        "    image_width = 32\n",
        "    image_depth = 3\n",
        "    image_bytes = image_height * image_width * image_depth\n",
        "    record_bytes = label_bytes + image_bytes\n",
        "\n",
        "    def _parse_record(value):\n",
        "        raw_data = tf.decode_raw(value, tf.uint8)\n",
        "        raw_data.set_shape([record_bytes])\n",
        "        label = tf.squeeze(tf.cast(raw_data[:label_bytes], tf.int32), axis=0)\n",
        "        depth_major = tf.reshape(\n",
        "            raw_data[label_bytes:],\n",
        "            [image_depth, image_height, image_width])\n",
        "        image = tf.transpose(depth_major, [1, 2, 0])\n",
        "        image = tf.to_float(image) / 255\n",
        "        if augment:\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "            image = tf.image.random_brightness(image, max_delta=0.2)\n",
        "            image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
        "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "        features = {'image': image}\n",
        "        labels = {'label': label}\n",
        "        return features, labels\n",
        "        \n",
        "    def _input_fn():\n",
        "        with tf.device('/cpu:0'):\n",
        "            dataset = tf.data.FixedLengthRecordDataset(filenames, record_bytes)\n",
        "            dataset = dataset.map(\n",
        "                map_func=_parse_record,\n",
        "                num_parallel_calls=4)\n",
        "            if shuffle:\n",
        "                dataset = dataset.shuffle(buffer_size=10000)\n",
        "            dataset = dataset.batch(batch_size)\n",
        "            dataset = dataset.prefetch(buffer_size=1000)\n",
        "            dataset = dataset.repeat(num_epochs)\n",
        "            iterator = dataset.make_one_shot_iterator()\n",
        "            return iterator.get_next()\n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbF8GxfvMs5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "5b278d8d-fa61-49d7-c455-998204bd632b"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    input_fn = get_input_fn(\n",
        "        filenames=[os.path.join(data_dir, 'test_batch.bin')],\n",
        "        batch_size=16,\n",
        "        num_epochs=1,\n",
        "        shuffle=False,\n",
        "        augment=True)\n",
        "\n",
        "    features, labels = input_fn()\n",
        "    image_batch, label_batch = sess.run([features['image'], labels['label']])\n",
        "    print('Images Min:', image_batch.min(), 'Max:', image_batch.max())\n",
        "\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "    for i, ax in enumerate(axes.ravel()):\n",
        "        ax.imshow(image_batch[i])\n",
        "        ax.grid(False)\n",
        "        ax.axis('off')\n",
        "        ax.set_title('Label: {}'.format(label_batch[i]))\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images Min: 0.0 Max: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiUAAAI0CAYAAADC51nVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvZQYDAaDwWCYC9hLicFgMBgMhrnA/wMC5Zn0bzsAGwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5c857df990>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "v9ih6KgrLCgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "RbGhOg7vkwPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_norm(inputs, momentum, training):\n",
        "    return tf.layers.batch_normalization(\n",
        "        inputs=inputs,\n",
        "        scale=False,\n",
        "        momentum=momentum,\n",
        "        training=training,\n",
        "        fused=True)\n",
        "\n",
        "def resnet_block(inputs, filters, strides, training, params):\n",
        "    kernel_initializer = tf.initializers.variance_scaling(scale=2.0)\n",
        "    kernel_regularizer = tf.contrib.layers.l2_regularizer(scale=params['l2_scale'])\n",
        "\n",
        "    inputs = batch_norm(\n",
        "        inputs=inputs,\n",
        "        momentum=params['bn_momentum'],\n",
        "        training=training)\n",
        "    inputs = tf.nn.relu(inputs)\n",
        "\n",
        "    hidden = tf.layers.conv2d(\n",
        "        inputs=inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=3,\n",
        "        strides=strides,\n",
        "        use_bias=False,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        padding='same')\n",
        "\n",
        "    hidden = batch_norm(\n",
        "        inputs=hidden,\n",
        "        momentum=params['bn_momentum'],\n",
        "        training=training)\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "    hidden = tf.layers.conv2d(\n",
        "        inputs=hidden,\n",
        "        filters=filters,\n",
        "        kernel_size=3,\n",
        "        strides=1,\n",
        "        use_bias=False,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        padding='same')\n",
        "\n",
        "    inputs_padded = tf.layers.conv2d(\n",
        "        inputs=inputs,\n",
        "        filters=filters,\n",
        "        kernel_size=1,\n",
        "        strides=strides,\n",
        "        use_bias=False,\n",
        "        padding='same',\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        kernel_regularizer=kernel_regularizer)\n",
        "    hidden = hidden + inputs_padded\n",
        "    \n",
        "    return hidden\n",
        "\n",
        "def resnet_layer(inputs, filters, strides, training, params):\n",
        "    hidden = resnet_block(inputs, filters, strides, training, params)\n",
        "    hidden = resnet_block(hidden, filters, 1, training, params)\n",
        "    hidden = resnet_block(hidden, filters, 1, training, params)\n",
        "    hidden = resnet_block(hidden, filters, 1, training, params)\n",
        "    hidden = resnet_block(hidden, filters, 1, training, params)\n",
        "    hidden = resnet_block(hidden, filters, 1, training, params)\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ch-hDue_LEu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    kernel_initializer = tf.initializers.variance_scaling(scale=2.0)\n",
        "    kernel_regularizer = tf.contrib.layers.l2_regularizer(scale=params['l2_scale'])\n",
        "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    num_classes = 10\n",
        "\n",
        "    inputs = features['image'] * 2 - 1\n",
        "\n",
        "    hidden = tf.layers.conv2d(\n",
        "        inputs=inputs,\n",
        "        filters=16,\n",
        "        kernel_size=3,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        padding='same')\n",
        "\n",
        "    hidden = resnet_layer(hidden, filters=16, strides=2, training=training, params=params)\n",
        "    #hidden = resnet_layer(hidden, filters=16, strides=1, training=training, params=params)\n",
        "    #hidden = resnet_layer(hidden, filters=16, strides=1, training=training, params=params)\n",
        "    #hidden = resnet_layer(hidden, filters=32, strides=1, training=training, params=params)\n",
        "    #hidden = resnet_layer(hidden, filters=64, strides=1, training=training, params=params)\n",
        "\n",
        "    hidden = batch_norm(\n",
        "        inputs=hidden,\n",
        "        momentum=params['bn_momentum'],\n",
        "        training=training)\n",
        "    hidden = tf.nn.relu(hidden)\n",
        "\n",
        "    hidden = tf.reduce_mean(hidden, axis=[1, 2])\n",
        "\n",
        "    logits = tf.layers.dense(\n",
        "        inputs=hidden,\n",
        "        units=num_classes,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "    outputs = tf.nn.softmax(logits)\n",
        "    prediction = tf.argmax(outputs, axis=-1)\n",
        "    \n",
        "    predictions = {\n",
        "        'prediction': prediction\n",
        "    }\n",
        "\n",
        "    loss = None\n",
        "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
        "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
        "            labels=labels['label'],\n",
        "            logits=logits)\n",
        "        loss = tf.losses.get_total_loss()\n",
        "        \n",
        "    train_op = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        global_step = tf.train.get_or_create_global_step()\n",
        "        learning_rate = tf.train.piecewise_constant(\n",
        "            boundaries=params['lr_boundaries'],\n",
        "            values=params['lr_values'],\n",
        "            x=global_step)\n",
        "\n",
        "        optimizer = tf.train.MomentumOptimizer(\n",
        "            learning_rate=learning_rate,\n",
        "            momentum=params['momentum'])\n",
        "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "            train_op = optimizer.minimize(\n",
        "                global_step=global_step,\n",
        "                loss=loss)\n",
        "\n",
        "    eval_metric_ops = None\n",
        "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
        "        eval_metric_ops = {\n",
        "            'accuracy': tf.metrics.accuracy(\n",
        "                predictions=predictions['prediction'],\n",
        "                labels=labels['label'])\n",
        "        }\n",
        "\n",
        "    estimator_spec = tf.estimator.EstimatorSpec(\n",
        "        loss=loss,\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        predictions=predictions,\n",
        "        eval_metric_ops=eval_metric_ops)\n",
        "    return estimator_spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zx_enNtMKsuH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "nvBISqbuDu1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 23956
        },
        "outputId": "c42d49de-845a-48d5-d96b-cb0e14a39444"
      },
      "cell_type": "code",
      "source": [
        "max_steps_train = 10000\n",
        "batch_size = 256\n",
        "model_dir = os.path.join('jobs/', str(int(time.time())))\n",
        "print('Model Directory:', model_dir)\n",
        "\n",
        "params = {\n",
        "    'l2_scale': 1e-4,\n",
        "    'lr_values': [1e-1, 1e-2, 1e-3],\n",
        "    'lr_boundaries': [max_steps_train // 3, max_steps_train // 3 * 2],\n",
        "    'bn_momentum': 0.99,\n",
        "    'momentum': 0.9,\n",
        "}\n",
        "print('Hyperparameters:', params)\n",
        "\n",
        "config = tf.estimator.RunConfig(\n",
        "    save_checkpoints_steps=1000,\n",
        "    save_checkpoints_secs=None,\n",
        "    save_summary_steps=100)\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    model_dir=model_dir,\n",
        "    config=config,\n",
        "    params=params)\n",
        "\n",
        "filenames_train = [\n",
        "    os.path.join(data_dir, 'data_batch_{}.bin'.format(i))\n",
        "    for i in xrange(1, 6)]\n",
        "filenames_eval = [os.path.join(data_dir, 'test_batch.bin')]\n",
        "\n",
        "input_fn_train = get_input_fn(\n",
        "    filenames=filenames_train,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=1,\n",
        "    shuffle=True,\n",
        "    augment=True)\n",
        "input_fn_eval = get_input_fn(\n",
        "    filenames=filenames_eval,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=1,\n",
        "    shuffle=False,\n",
        "    augment=False)\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn=input_fn_train,\n",
        "    max_steps=max_steps_train)\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn=input_fn_eval,\n",
        "    start_delay_secs=0,\n",
        "    throttle_secs=60 * 5)\n",
        "\n",
        "tf.estimator.train_and_evaluate(\n",
        "    train_spec=train_spec,\n",
        "    eval_spec=eval_spec,\n",
        "    estimator=estimator)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Directory: jobs/1531952916\n",
            "Hyperparameters: {'lr_boundaries': [3333, 6666], 'momentum': 0.9, 'bn_momentum': 0.99, 'l2_scale': 0.0001, 'lr_values': [0.1, 0.01, 0.001]}\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb1720fa0d0>, '_evaluation_master': '', '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'jobs/1531952916', '_train_distribute': None, '_save_summary_steps': 100}\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 300 secs (eval_spec.throttle_secs) or training is finished.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4838855, step = 0\n",
            "INFO:tensorflow:global_step/sec: 11.3384\n",
            "INFO:tensorflow:loss = 1.8623706, step = 100 (8.827 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 196 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.7237809.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:29:00\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-196\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:29:02\n",
            "INFO:tensorflow:Saving dict for global step 196: accuracy = 0.2678, global_step = 196, loss = 2.6064448\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 196: jobs/1531952916/model.ckpt-196\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-196\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 196 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.7155201, step = 196\n",
            "INFO:tensorflow:global_step/sec: 11.4069\n",
            "INFO:tensorflow:loss = 1.5001843, step = 296 (8.770 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 392 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.3162559.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:29:26\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-392\n",
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:29:29\n",
            "INFO:tensorflow:Saving dict for global step 392: accuracy = 0.3945, global_step = 392, loss = 1.7490101\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 392: jobs/1531952916/model.ckpt-392\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-392\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 392 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.4242495, step = 392\n",
            "INFO:tensorflow:global_step/sec: 11.5145\n",
            "INFO:tensorflow:loss = 1.4302735, step = 492 (8.687 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 588 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.1255481.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:29:52\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-588\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:29:54\n",
            "INFO:tensorflow:Saving dict for global step 588: accuracy = 0.4239, global_step = 588, loss = 1.7936857\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 588: jobs/1531952916/model.ckpt-588\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-588\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 588 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.309706, step = 588\n",
            "INFO:tensorflow:global_step/sec: 11.4844\n",
            "INFO:tensorflow:loss = 1.28865, step = 688 (8.710 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 784 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.2341523.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:30:18\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-784\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:30:20\n",
            "INFO:tensorflow:Saving dict for global step 784: accuracy = 0.4168, global_step = 784, loss = 1.7930431\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 784: jobs/1531952916/model.ckpt-784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-784\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 784 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.0909792, step = 784\n",
            "INFO:tensorflow:global_step/sec: 11.454\n",
            "INFO:tensorflow:loss = 1.1268712, step = 884 (8.738 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 980 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.4879646.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:30:43\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-980\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:30:46\n",
            "INFO:tensorflow:Saving dict for global step 980: accuracy = 0.3837, global_step = 980, loss = 2.1597047\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 980: jobs/1531952916/model.ckpt-980\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-980\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 980 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 1.1241015, step = 980\n",
            "INFO:tensorflow:global_step/sec: 11.3692\n",
            "INFO:tensorflow:loss = 1.0107336, step = 1080 (8.796 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1176 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.3549275.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:31:09\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1176\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:31:12\n",
            "INFO:tensorflow:Saving dict for global step 1176: accuracy = 0.5478, global_step = 1176, loss = 1.3173625\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1176: jobs/1531952916/model.ckpt-1176\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1176\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1176 into jobs/1531952916/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.0860685, step = 1176\n",
            "INFO:tensorflow:global_step/sec: 11.3388\n",
            "INFO:tensorflow:loss = 1.0931851, step = 1276 (8.824 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1372 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.064633.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:31:35\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1372\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:31:38\n",
            "INFO:tensorflow:Saving dict for global step 1372: accuracy = 0.6232, global_step = 1372, loss = 1.096455\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1372: jobs/1531952916/model.ckpt-1372\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1372\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1372 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.9806318, step = 1372\n",
            "INFO:tensorflow:global_step/sec: 11.3618\n",
            "INFO:tensorflow:loss = 0.99406123, step = 1472 (8.806 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1568 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.0858879.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:32:01\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1568\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:32:04\n",
            "INFO:tensorflow:Saving dict for global step 1568: accuracy = 0.5825, global_step = 1568, loss = 1.3200023\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1568: jobs/1531952916/model.ckpt-1568\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1568\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1568 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.9181829, step = 1568\n",
            "INFO:tensorflow:global_step/sec: 11.4091\n",
            "INFO:tensorflow:loss = 0.9378718, step = 1668 (8.767 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1764 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.1040463.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:32:27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1764\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:32:30\n",
            "INFO:tensorflow:Saving dict for global step 1764: accuracy = 0.6119, global_step = 1764, loss = 1.1428801\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1764: jobs/1531952916/model.ckpt-1764\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1764\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1764 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8880346, step = 1764\n",
            "INFO:tensorflow:global_step/sec: 11.0727\n",
            "INFO:tensorflow:loss = 0.9848317, step = 1864 (9.041 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1960 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.0743121.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:32:54\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1960\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:32:57\n",
            "INFO:tensorflow:Saving dict for global step 1960: accuracy = 0.6108, global_step = 1960, loss = 1.2004073\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1960: jobs/1531952916/model.ckpt-1960\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-1960\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1960 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8654091, step = 1960\n",
            "INFO:tensorflow:global_step/sec: 11.1339\n",
            "INFO:tensorflow:loss = 0.91284686, step = 2060 (8.989 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2156 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.9257901.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:33:21\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2156\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:33:23\n",
            "INFO:tensorflow:Saving dict for global step 2156: accuracy = 0.6415, global_step = 2156, loss = 1.1274569\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2156: jobs/1531952916/model.ckpt-2156\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2156\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 2156 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.94911015, step = 2156\n",
            "INFO:tensorflow:global_step/sec: 11.209\n",
            "INFO:tensorflow:loss = 0.8609535, step = 2256 (8.932 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2352 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.87007284.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:33:47\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2352\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:33:49\n",
            "INFO:tensorflow:Saving dict for global step 2352: accuracy = 0.5574, global_step = 2352, loss = 1.4971086\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2352: jobs/1531952916/model.ckpt-2352\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2352\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 2352 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.83129436, step = 2352\n",
            "INFO:tensorflow:global_step/sec: 11.214\n",
            "INFO:tensorflow:loss = 0.8424908, step = 2452 (8.924 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2548 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.99488956.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:34:13\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2548\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:34:16\n",
            "INFO:tensorflow:Saving dict for global step 2548: accuracy = 0.5923, global_step = 2548, loss = 1.2805121\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2548: jobs/1531952916/model.ckpt-2548\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2548\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 2548 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8279925, step = 2548\n",
            "INFO:tensorflow:global_step/sec: 11.1867\n",
            "INFO:tensorflow:loss = 0.93484825, step = 2648 (8.941 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2744 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7830824.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:34:40\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2744\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:34:43\n",
            "INFO:tensorflow:Saving dict for global step 2744: accuracy = 0.6179, global_step = 2744, loss = 1.1846114\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2744: jobs/1531952916/model.ckpt-2744\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2744\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 2744 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8832915, step = 2744\n",
            "INFO:tensorflow:global_step/sec: 11.2838\n",
            "INFO:tensorflow:loss = 0.9257704, step = 2844 (8.872 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2940 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.96155787.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:35:06\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2940\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:35:09\n",
            "INFO:tensorflow:Saving dict for global step 2940: accuracy = 0.6513, global_step = 2940, loss = 1.0692822\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2940: jobs/1531952916/model.ckpt-2940\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-2940\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 2940 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8055238, step = 2940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 11.1435\n",
            "INFO:tensorflow:loss = 0.8387805, step = 3040 (8.983 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3136 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.78830624.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:35:33\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3136\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:35:35\n",
            "INFO:tensorflow:Saving dict for global step 3136: accuracy = 0.523, global_step = 3136, loss = 1.601244\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3136: jobs/1531952916/model.ckpt-3136\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3136\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 3136 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.80289876, step = 3136\n",
            "INFO:tensorflow:global_step/sec: 11.1908\n",
            "INFO:tensorflow:loss = 0.91529673, step = 3236 (8.941 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3332 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.8893519.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:35:59\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3332\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:36:02\n",
            "INFO:tensorflow:Saving dict for global step 3332: accuracy = 0.6052, global_step = 3332, loss = 1.2646499\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3332: jobs/1531952916/model.ckpt-3332\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3332\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 3332 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.73463625, step = 3332\n",
            "INFO:tensorflow:global_step/sec: 11.19\n",
            "INFO:tensorflow:loss = 0.7580766, step = 3432 (8.944 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3528 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7243248.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:36:26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3528\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:36:28\n",
            "INFO:tensorflow:Saving dict for global step 3528: accuracy = 0.7604, global_step = 3528, loss = 0.76718336\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3528: jobs/1531952916/model.ckpt-3528\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3528\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 3528 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7788293, step = 3528\n",
            "INFO:tensorflow:global_step/sec: 11.2225\n",
            "INFO:tensorflow:loss = 0.73244584, step = 3628 (8.921 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3724 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.8578875.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:36:52\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3724\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:36:55\n",
            "INFO:tensorflow:Saving dict for global step 3724: accuracy = 0.7572, global_step = 3724, loss = 0.7762702\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3724: jobs/1531952916/model.ckpt-3724\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3724\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 3724 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6998098, step = 3724\n",
            "INFO:tensorflow:global_step/sec: 11.2464\n",
            "INFO:tensorflow:loss = 0.74538773, step = 3824 (8.893 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3920 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.8896414.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:37:18\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3920\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:37:21\n",
            "INFO:tensorflow:Saving dict for global step 3920: accuracy = 0.76, global_step = 3920, loss = 0.75979644\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3920: jobs/1531952916/model.ckpt-3920\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-3920\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 3920 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6686795, step = 3920\n",
            "INFO:tensorflow:global_step/sec: 11.2101\n",
            "INFO:tensorflow:loss = 0.6853267, step = 4020 (8.922 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4116 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.000063.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:37:45\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4116\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:37:48\n",
            "INFO:tensorflow:Saving dict for global step 4116: accuracy = 0.7624, global_step = 4116, loss = 0.7551577\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4116: jobs/1531952916/model.ckpt-4116\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4116\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4116 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7034513, step = 4116\n",
            "INFO:tensorflow:global_step/sec: 11.2708\n",
            "INFO:tensorflow:loss = 0.6725441, step = 4216 (8.883 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4312 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.722518.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:38:11\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4312\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:38:14\n",
            "INFO:tensorflow:Saving dict for global step 4312: accuracy = 0.7594, global_step = 4312, loss = 0.7651348\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4312: jobs/1531952916/model.ckpt-4312\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4312\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4312 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7040085, step = 4312\n",
            "INFO:tensorflow:global_step/sec: 11.3348\n",
            "INFO:tensorflow:loss = 0.7605391, step = 4412 (8.828 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4508 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7755406.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:38:38\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4508\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:38:40\n",
            "INFO:tensorflow:Saving dict for global step 4508: accuracy = 0.7659, global_step = 4508, loss = 0.7505754\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4508: jobs/1531952916/model.ckpt-4508\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4508\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4508 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7666167, step = 4508\n",
            "INFO:tensorflow:global_step/sec: 11.2598\n",
            "INFO:tensorflow:loss = 0.76293063, step = 4608 (8.885 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4704 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.77649844.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:39:04\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4704\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:39:06\n",
            "INFO:tensorflow:Saving dict for global step 4704: accuracy = 0.7596, global_step = 4704, loss = 0.76321745\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4704: jobs/1531952916/model.ckpt-4704\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4704\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4704 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.76077443, step = 4704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 11.377\n",
            "INFO:tensorflow:loss = 0.67542446, step = 4804 (8.800 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4900 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.66576976.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:39:30\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4900\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:39:32\n",
            "INFO:tensorflow:Saving dict for global step 4900: accuracy = 0.7682, global_step = 4900, loss = 0.74170864\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4900: jobs/1531952916/model.ckpt-4900\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-4900\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4900 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6667714, step = 4900\n",
            "INFO:tensorflow:global_step/sec: 11.2436\n",
            "INFO:tensorflow:loss = 0.6580727, step = 5000 (8.902 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5096 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.49480826.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:39:56\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5096\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:39:58\n",
            "INFO:tensorflow:Saving dict for global step 5096: accuracy = 0.7617, global_step = 5096, loss = 0.7556547\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5096: jobs/1531952916/model.ckpt-5096\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5096\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5096 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7001108, step = 5096\n",
            "INFO:tensorflow:global_step/sec: 11.2269\n",
            "INFO:tensorflow:loss = 0.7415611, step = 5196 (8.911 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5292 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.575736.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:40:22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5292\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:40:25\n",
            "INFO:tensorflow:Saving dict for global step 5292: accuracy = 0.7619, global_step = 5292, loss = 0.7607501\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5292: jobs/1531952916/model.ckpt-5292\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5292\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5292 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.8782901, step = 5292\n",
            "INFO:tensorflow:global_step/sec: 11.2836\n",
            "INFO:tensorflow:loss = 0.699166, step = 5392 (8.872 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5488 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7202938.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:40:48\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5488\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:40:51\n",
            "INFO:tensorflow:Saving dict for global step 5488: accuracy = 0.7638, global_step = 5488, loss = 0.751526\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5488: jobs/1531952916/model.ckpt-5488\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5488\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5488 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.65738666, step = 5488\n",
            "INFO:tensorflow:global_step/sec: 11.3027\n",
            "INFO:tensorflow:loss = 0.66434574, step = 5588 (8.857 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5684 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.71894705.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:41:15\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5684\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:41:17\n",
            "INFO:tensorflow:Saving dict for global step 5684: accuracy = 0.7613, global_step = 5684, loss = 0.7622845\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5684: jobs/1531952916/model.ckpt-5684\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5684\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5684 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6320765, step = 5684\n",
            "INFO:tensorflow:global_step/sec: 11.2459\n",
            "INFO:tensorflow:loss = 0.6329513, step = 5784 (8.900 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5880 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.6165414.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:41:41\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5880\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:41:43\n",
            "INFO:tensorflow:Saving dict for global step 5880: accuracy = 0.7668, global_step = 5880, loss = 0.7388958\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5880: jobs/1531952916/model.ckpt-5880\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-5880\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5880 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.65292215, step = 5880\n",
            "INFO:tensorflow:global_step/sec: 11.3646\n",
            "INFO:tensorflow:loss = 0.62946296, step = 5980 (8.811 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6076 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.82710797.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:42:07\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6076\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:42:10\n",
            "INFO:tensorflow:Saving dict for global step 6076: accuracy = 0.7682, global_step = 6076, loss = 0.7320436\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6076: jobs/1531952916/model.ckpt-6076\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6076\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 6076 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.71315354, step = 6076\n",
            "INFO:tensorflow:global_step/sec: 11.2546\n",
            "INFO:tensorflow:loss = 0.75931525, step = 6176 (8.896 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6272 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.8739346.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:42:34\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6272\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:42:36\n",
            "INFO:tensorflow:Saving dict for global step 6272: accuracy = 0.7643, global_step = 6272, loss = 0.7580651\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6272: jobs/1531952916/model.ckpt-6272\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6272\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 6272 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.71093273, step = 6272\n",
            "INFO:tensorflow:global_step/sec: 11.324\n",
            "INFO:tensorflow:loss = 0.58646804, step = 6372 (8.844 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6468 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.77834177.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:43:00\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6468\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:43:03\n",
            "INFO:tensorflow:Saving dict for global step 6468: accuracy = 0.7721, global_step = 6468, loss = 0.7300981\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6468: jobs/1531952916/model.ckpt-6468\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6468\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 6468 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6076568, step = 6468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 11.3089\n",
            "INFO:tensorflow:loss = 0.73263127, step = 6568 (8.852 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6664 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.9045149.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:43:26\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6664\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:43:29\n",
            "INFO:tensorflow:Saving dict for global step 6664: accuracy = 0.7686, global_step = 6664, loss = 0.73358506\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6664: jobs/1531952916/model.ckpt-6664\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6664\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 6664 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6542358, step = 6664\n",
            "INFO:tensorflow:global_step/sec: 11.3008\n",
            "INFO:tensorflow:loss = 0.68739146, step = 6764 (8.859 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6860 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7286077.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:43:52\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6860\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:43:55\n",
            "INFO:tensorflow:Saving dict for global step 6860: accuracy = 0.7752, global_step = 6860, loss = 0.720822\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6860: jobs/1531952916/model.ckpt-6860\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-6860\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 6860 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7054318, step = 6860\n",
            "INFO:tensorflow:global_step/sec: 11.2111\n",
            "INFO:tensorflow:loss = 0.6499827, step = 6960 (8.923 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7056 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.5435903.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:44:18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7056\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:44:21\n",
            "INFO:tensorflow:Saving dict for global step 7056: accuracy = 0.7744, global_step = 7056, loss = 0.71964824\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7056: jobs/1531952916/model.ckpt-7056\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7056\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 7056 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.60164243, step = 7056\n",
            "INFO:tensorflow:global_step/sec: 11.4623\n",
            "INFO:tensorflow:loss = 0.7527237, step = 7156 (8.732 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7252 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.74351424.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:44:45\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7252\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:44:47\n",
            "INFO:tensorflow:Saving dict for global step 7252: accuracy = 0.7744, global_step = 7252, loss = 0.72092867\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7252: jobs/1531952916/model.ckpt-7252\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7252\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 7252 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.71814114, step = 7252\n",
            "INFO:tensorflow:global_step/sec: 11.3177\n",
            "INFO:tensorflow:loss = 0.61679566, step = 7352 (8.843 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7448 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.64144266.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:45:11\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7448\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:45:13\n",
            "INFO:tensorflow:Saving dict for global step 7448: accuracy = 0.7757, global_step = 7448, loss = 0.7188652\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7448: jobs/1531952916/model.ckpt-7448\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7448\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 7448 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.60641044, step = 7448\n",
            "INFO:tensorflow:global_step/sec: 11.3303\n",
            "INFO:tensorflow:loss = 0.6246575, step = 7548 (8.835 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7644 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.73035544.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:45:37\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7644\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:45:39\n",
            "INFO:tensorflow:Saving dict for global step 7644: accuracy = 0.776, global_step = 7644, loss = 0.71858835\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7644: jobs/1531952916/model.ckpt-7644\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7644\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 7644 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6091725, step = 7644\n",
            "INFO:tensorflow:global_step/sec: 11.345\n",
            "INFO:tensorflow:loss = 0.6023042, step = 7744 (8.820 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7840 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.74186945.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:46:03\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7840\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:46:05\n",
            "INFO:tensorflow:Saving dict for global step 7840: accuracy = 0.7748, global_step = 7840, loss = 0.7185103\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7840: jobs/1531952916/model.ckpt-7840\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-7840\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 7840 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.7322754, step = 7840\n",
            "INFO:tensorflow:global_step/sec: 11.5076\n",
            "INFO:tensorflow:loss = 0.5744172, step = 7940 (8.698 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8036 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.6006839.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:46:29\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8036\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:46:31\n",
            "INFO:tensorflow:Saving dict for global step 8036: accuracy = 0.7753, global_step = 8036, loss = 0.71818984\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8036: jobs/1531952916/model.ckpt-8036\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8036\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 8036 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.71481156, step = 8036\n",
            "INFO:tensorflow:global_step/sec: 11.4033\n",
            "INFO:tensorflow:loss = 0.64786875, step = 8136 (8.780 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8232 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.72856206.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:46:55\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8232\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:46:57\n",
            "INFO:tensorflow:Saving dict for global step 8232: accuracy = 0.7761, global_step = 8232, loss = 0.7175159\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8232: jobs/1531952916/model.ckpt-8232\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8232\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 8232 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6018759, step = 8232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 11.5032\n",
            "INFO:tensorflow:loss = 0.66978884, step = 8332 (8.696 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8428 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.85994166.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:47:21\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8428\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:47:23\n",
            "INFO:tensorflow:Saving dict for global step 8428: accuracy = 0.775, global_step = 8428, loss = 0.71872616\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8428: jobs/1531952916/model.ckpt-8428\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8428\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 8428 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6198184, step = 8428\n",
            "INFO:tensorflow:global_step/sec: 11.317\n",
            "INFO:tensorflow:loss = 0.6748287, step = 8528 (8.845 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8624 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.73772365.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:47:47\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8624\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:47:49\n",
            "INFO:tensorflow:Saving dict for global step 8624: accuracy = 0.7743, global_step = 8624, loss = 0.71721274\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8624: jobs/1531952916/model.ckpt-8624\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8624\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 8624 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.751155, step = 8624\n",
            "INFO:tensorflow:global_step/sec: 11.4255\n",
            "INFO:tensorflow:loss = 0.7599427, step = 8724 (8.755 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8820 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.80686605.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:48:13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8820\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:48:16\n",
            "INFO:tensorflow:Saving dict for global step 8820: accuracy = 0.7753, global_step = 8820, loss = 0.71855104\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8820: jobs/1531952916/model.ckpt-8820\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-8820\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 8820 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.65172076, step = 8820\n",
            "INFO:tensorflow:global_step/sec: 11.4189\n",
            "INFO:tensorflow:loss = 0.6689043, step = 8920 (8.763 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9016 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.6292126.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:48:39\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9016\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:48:42\n",
            "INFO:tensorflow:Saving dict for global step 9016: accuracy = 0.7758, global_step = 9016, loss = 0.71686506\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9016: jobs/1531952916/model.ckpt-9016\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9016\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9016 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.68507534, step = 9016\n",
            "INFO:tensorflow:global_step/sec: 11.5701\n",
            "INFO:tensorflow:loss = 0.65485245, step = 9116 (8.649 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9212 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.9812986.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:49:05\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9212\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:49:07\n",
            "INFO:tensorflow:Saving dict for global step 9212: accuracy = 0.777, global_step = 9212, loss = 0.7170502\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9212: jobs/1531952916/model.ckpt-9212\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9212\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9212 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.74107033, step = 9212\n",
            "INFO:tensorflow:global_step/sec: 11.4328\n",
            "INFO:tensorflow:loss = 0.6439933, step = 9312 (8.758 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9408 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7595264.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:49:31\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9408\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:49:33\n",
            "INFO:tensorflow:Saving dict for global step 9408: accuracy = 0.7768, global_step = 9408, loss = 0.7159999\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9408: jobs/1531952916/model.ckpt-9408\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9408\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9408 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6051242, step = 9408\n",
            "INFO:tensorflow:global_step/sec: 11.5405\n",
            "INFO:tensorflow:loss = 0.59701514, step = 9508 (8.673 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9604 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.5334604.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:49:57\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9604\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:49:59\n",
            "INFO:tensorflow:Saving dict for global step 9604: accuracy = 0.7758, global_step = 9604, loss = 0.7184809\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9604: jobs/1531952916/model.ckpt-9604\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9604\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9604 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.6907961, step = 9604\n",
            "INFO:tensorflow:global_step/sec: 11.3953\n",
            "INFO:tensorflow:loss = 0.54340196, step = 9704 (8.785 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9800 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.7785628.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:50:23\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9800\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:50:25\n",
            "INFO:tensorflow:Saving dict for global step 9800: accuracy = 0.7765, global_step = 9800, loss = 0.7150383\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9800: jobs/1531952916/model.ckpt-9800\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9800\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9800 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.64344233, step = 9800\n",
            "INFO:tensorflow:global_step/sec: 11.4914\n",
            "INFO:tensorflow:loss = 0.81087255, step = 9900 (8.708 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9996 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.6194094.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:50:48\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9996\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:50:51\n",
            "INFO:tensorflow:Saving dict for global step 9996: accuracy = 0.7762, global_step = 9996, loss = 0.71520317\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 9996: jobs/1531952916/model.ckpt-9996\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-9996\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 9996 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.70071614, step = 9996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 10000 into jobs/1531952916/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.63786316.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-07-18-22:51:02\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from jobs/1531952916/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Finished evaluation at 2018-07-18-22:51:04\n",
            "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.7758, global_step = 10000, loss = 0.71549886\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: jobs/1531952916/model.ckpt-10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': 0.7758, 'global_step': 10000, 'loss': 0.71549886}, [])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "otL8RWiLDy4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download\n",
        "\n",
        "You can download the model directory for inspection via TensorBoard."
      ]
    },
    {
      "metadata": {
        "id": "7qjPqmKV_Qtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ec5b777-4ca5-422b-8304-f91f3e9a2f16"
      },
      "cell_type": "code",
      "source": [
        "model_tarball = model_dir + '.tar.gz'\n",
        "with tarfile.open(model_tarball, \"w:gz\") as tar:\n",
        "    tar.add(model_dir, arcname=os.path.basename(model_dir))\n",
        "files.download(model_tarball)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobs/1522767533.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ahVqpu2Sonh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary Overview: \n",
        "\n",
        "  The observations below are only my observation based on the original given code. I modified some paramaters and the following is my observation of these modifications. I implimented a different Resnet V2 and also a simple convolution net and my code is provided in two seperate ipython files. \n",
        "\n",
        "  First I tried data augmentation to the images, based on the initial setup that was done in code like random flipping the images, britness adjustements ...etc. But it did not change the accuracy, which I suspected at least an extra boast of at least 3% but that was not the case. I suspected maybe the model needs extra trading so I increased the number of steps and epoches.  \n",
        "\n",
        "  I tried increasing the epochs and the number of steps which is expected to increase accuracy. But there was not a significant change in the result. So I started to think about what could be wrong in the architecture. \n",
        "\n",
        "  Having a look at the code, I believe what is intended to achieve from this code is to develop a ResNet V2 like the one in [1] , however, I can see that the downsampling that was implemented in the code (strides) and the skip connection is not properly done like explained in the literatire. Now this could be a problem or it may nesseraly be a type of an architecture that I am not familiar with. \n",
        "\n",
        "  Comparing the implementation of the paper and the given implementaiton in this code I do not see an obvious fundamental mistake of how a resnet network should look like. But a resnet with a deep architecture like this one should give a result more than 90% accuracy on this dataset.\n",
        "\n",
        "  I wondered to see if there could be a hidden problem with the way the skip connection is done or the downsampling of the network. At first I tried a simple convolution architecture apporach with only two layers of conv net with reul and max polling (without skip connection or residual inputs) and the result was 78% accuracy. The implementation is shown in the upcoming sections. Also, it is worth to mention that I wanted to see if I add a mean subtraction step to the data after standardizing it to the highest pixel value and it turns out that the resule was 79% accurate. Even though the difference is not that sigificant but it is worth to mention that the convergence was much faster. \n",
        " \n",
        "  It is expected that a much more deep architecture gives a higher accuracy result, but also a deep archictecture comes with problems such as exploding and vanishing gradients. The residual architecture should address this problem by doing the skip connection trick, however, it is also very sensitive and tricky to implement. Since it is expected that the more the epoches, steps and the deeper the resnet layers the more the accuracy but the implementation provided does not show this. This maked me a little scheptical about how the skip connections are connected between stacks and residual blocks also how the downsampling (strides = 2) is done. For this reason I tried to to reduce the number of residual layers to only one layer and see if the results improve. The moment I commented the lines (removing the last 4 resnet layers in the code) which means it will only run one layer the accuracy has jumped from 57% to 78% at 1000 steps. This confirms it that the residual blocks are not rerouting the residual inputs properly. From my previous experience with residual networks this can be very hard to trace manually in the code so it will be easier to reimplement the code again based on the paper [1] I mentioned above to try something else and they showed some good results so I will try to replicate that. The implementation of this technique is in the following sections. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HfVMKTqXF00l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple Convolution Architecture\n",
        "\n",
        "The following is an implementation of a simple conv net with max pooling and dropouts to reduce over fitting. There are  2 layers of each has 2 convolution filters followed by a non-linear activation relu and a maxpooling layer. The model is then flatten to fit in a dense output layer of 10 units. \n",
        "\n",
        "Things I tried not in the code: \n",
        "\n",
        "- I tried adding a dense layer of 128 after flattening the model and before the output layer. I did this to see if it can improve the results by capturing intra correlations between the layers. This ended up adding extra model complexity as it dramatically increased the number of optimization params and it didnt improve the model accuracy. "
      ]
    },
    {
      "metadata": {
        "id": "RX10oUaBGhce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01f59d8c-4d13-46b2-a2b0-7fb30ac063d1"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pbt5lRj7n1rM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data and Variables "
      ]
    },
    {
      "metadata": {
        "id": "qlr7RCyTn1EH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fd819300-1688-4490-dd32-15839e96f2bf"
      },
      "cell_type": "code",
      "source": [
        "# retrain model or use saved model\n",
        "retrain_conv_net = False\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "num_classes = 10\n",
        "# mean subtraction (it improves the accuracy)\n",
        "mean_subtraction = True\n",
        "# directory to save models \n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models/')\n",
        "model_name_conv = 'conv_net.h5'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "# The data is already split between train and test sets. \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32') # convert to float\n",
        "x_test = x_test.astype('float32') # convert to float\n",
        "x_train /= 255 # Standarize to the maximum pixel value\n",
        "x_test /= 255 # Standarize to the maximum pixel value\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "if mean_subtraction:\n",
        "    train_mean = np.mean(x_train, axis=0)\n",
        "    x_train = x_train - train_mean\n",
        "    x_test  = x_test - train_mean \n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zqZuWh43oQ2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model and Training"
      ]
    },
    {
      "metadata": {
        "id": "lRUMdRyMoUyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "7af7a315-5510-49be-9b0d-eff7f10fe1e1"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.Adam(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_389 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_375 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_390 (Conv2D)          (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_376 (Activation)  (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_391 (Conv2D)          (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_377 (Activation)  (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_392 (Conv2D)          (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_378 (Activation)  (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                23050     \n",
            "_________________________________________________________________\n",
            "activation_379 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 88,618\n",
            "Trainable params: 88,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ucZzrqF7qFFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94944a22-1cc6-46cf-dd91-ed2998f79082"
      },
      "cell_type": "code",
      "source": [
        "if retrain_conv_net:  \n",
        "    histry = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
        "    model = histry.model\n",
        "    model.save_weights(save_dir+model_name_conv)\n",
        "else:\n",
        "    model.load_weights(save_dir+model_name_conv)\n",
        "    print ('Model loaded')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VH4ay0Wkqv_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5430518f-398b-4abb-d8f6-b164ef3b583a"
      },
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 236us/step\n",
            "Test loss: 0.5917723460197448\n",
            "Test accuracy: 0.8028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Bh4bC5Cq2Yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "c0b03517-2020-4ad9-9c23-2568e0c74ca3"
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "fig = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(histry.history['acc'])\n",
        "plt.plot(histry.history['val_acc'])\n",
        "plt.axhline(y=0.5, color='grey', linestyle='--')\n",
        "#plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "# summarize history for loss\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(histry.history['loss'])\n",
        "plt.plot(histry.history['val_loss'])\n",
        "plt.axhline(y=0.693, color='grey', linestyle='--')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig.savefig('learning_curve_CNN.png', bbox_inches='tight')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRpK3EEII0c/8f5H0MDyrhp2YAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5c463271d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "lbaGOGESrHDE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Observation and Discussion \n",
        "\n",
        "Surprzingly a simple architecture without residual learning can do a reasnably good job. I intentoinaly trained it on alot of epoches to observe when the model stars to overfit. As you can see the training and the testing error gap starts to widen after 100 epoches thats why the model starts to overfit after 100 epoches which can give about 78% accuracy (without mean subtraction) and 79% after 75 epoches with mean subtraction. A proper resnet architecture should surpase these results. However, it is expected to take so much time to train. Overall, it makes sense that mean subtraction improves the convergence speed as normalization of the data may lead to subtract any comman background noise in the data which may makes it easier to learn edges.  \n",
        "\n",
        "Another thing worth to mention is that I tried to add a dense layer after flattening the model at the final step to observe if it can capture correlations from the output of the convolution net. But this did not improve the accuracy of the model and added extra computational complexity without any impromevemnts so I decided to remove it. "
      ]
    },
    {
      "metadata": {
        "id": "axUpLmEzGi2N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configurable Resnet V2 implementation \n",
        "\n",
        "The following is an implementation of the paper [1] which is a resnetV2. The model consists of 3 stages, each stage consists of an equal number of blocks and each block contrains either 3 or 4 bottel neck. A bottel neck can either be a layer of batchnorm followed by a relu then a convolution layer, or it can be just a convolution layer on its own. The bottle neck is configrued to be only a convolution net when it is forming a new stage. At the begining of each stage the stride is set to be 2 to perform downsampling by 2 and the filter size is increased as well, that way the deeper convolution layers focuses more on the abstract structures of the images. \n",
        "\n",
        "The following code is configurable so that it can automatically builds deeper or shollower Resnet model with just adjusting the variable depth. Please note that depth should be multiples of this function f(n) = 9n+2 so it can be 20, 29, 56 or 110. \n",
        "\n",
        "The depth in the following code is set at 20, and it uses the same data preprocessing that was set in the last section. \n",
        "\n",
        "## Variables "
      ]
    },
    {
      "metadata": {
        "id": "reup0rLAsT-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# retrain model or use saved model\n",
        "retrain_resnet = False\n",
        "# Depth of resent V2 model \n",
        "depth = 20 #20,56,110,... 1001\n",
        "#save model_after_train \n",
        "model_name_resnet = 'resnetv2_trained.h5'\n",
        "# Data augmentation \n",
        "data_augmentation = True\n",
        "# Training parameters\n",
        "batch_size = 256\n",
        "epochs = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SW4RdQyRvHYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model:"
      ]
    },
    {
      "metadata": {
        "id": "lWVVYt8N9URr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2618
        },
        "outputId": "7c323c81-36a5-473e-bd8c-e5fd7ad576b7"
      },
      "cell_type": "code",
      "source": [
        "datagenerator = ImageDataGenerator(\n",
        "            # set input mean to 0 over the dataset\n",
        "            featurewise_center=False,\n",
        "            # set each sample mean to 0\n",
        "            samplewise_center=False,\n",
        "            # divide inputs by std of dataset\n",
        "            featurewise_std_normalization=False,\n",
        "            # divide each input by its std\n",
        "            samplewise_std_normalization=False,\n",
        "            # apply ZCA whitening\n",
        "            zca_whitening=False,\n",
        "            # epsilon for ZCA whitening\n",
        "            zca_epsilon=1e-06,\n",
        "            # randomly rotate images in the range (deg 0 to 180)\n",
        "            rotation_range=0,\n",
        "            # randomly shift images horizontally\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically\n",
        "            height_shift_range=0.1,\n",
        "            # set range for random shear\n",
        "            shear_range=0.,\n",
        "            # set range for random zoom\n",
        "            zoom_range=0.,\n",
        "            # set range for random channel shifts\n",
        "            channel_shift_range=0.,\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            # value used for fill_mode = \"constant\"\n",
        "            cval=0.,\n",
        "            # randomly flip images\n",
        "            horizontal_flip=True,\n",
        "            # randomly flip images\n",
        "            vertical_flip=False,\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "def learning_rate_adjuster(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    return lr\n",
        "\n",
        "def bottleneck(inputs, \n",
        "                no_filters=16,\n",
        "                kernel_size=3,\n",
        "                strides=1,\n",
        "                activation='relu',\n",
        "                batch_normalization=True):\n",
        "    x = inputs\n",
        "    if batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    x = Conv2D(no_filters,\n",
        "              kernel_size=kernel_size,\n",
        "              strides=strides,\n",
        "              padding='same',\n",
        "              kernel_initializer='he_normal',\n",
        "              kernel_regularizer=l2(1e-4))(x)\n",
        "    return x\n",
        "\n",
        "def builder_resnetv2(input_shape, depth, num_classes=10):\n",
        "    \"\"\" \n",
        "    This method build a resent v2 architecture based on the desired depth. \n",
        "    it builds stacks of bottleneck layers of Batch normalizer, relu then a convolution layer.  \n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 20, 29, 56 or 110 in [b])')\n",
        "    no_filters = 16\n",
        "    no_blocks = int((depth - 2) / 9)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Convolution layer with batch norm and relu on input before going deep in path\n",
        "    x = Conv2D(no_filters,\n",
        "                  kernel_size=3,\n",
        "                  strides=1,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    # build the residual blocks\n",
        "    for stage in range(3):\n",
        "        for block in range(no_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                no_filters_next = no_filters * 4\n",
        "                if block == 0: \n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                no_filters_next = no_filters * 2\n",
        "                if block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample by two by skipping 1\n",
        "\n",
        "            # bottleneck residual block\n",
        "            b = bottleneck(inputs=x,\n",
        "                             no_filters=no_filters,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization)\n",
        "            b = bottleneck(inputs=b,\n",
        "                             no_filters=no_filters)\n",
        "            b = bottleneck(inputs=b,\n",
        "                             no_filters=no_filters_next,\n",
        "                             kernel_size=1)\n",
        "            if block == 0:\n",
        "                # projects the residual skip connection to match the new dimensions\n",
        "                x = bottleneck(inputs=x,\n",
        "                                no_filters=no_filters_next,\n",
        "                                kernel_size=1,\n",
        "                                strides=strides,\n",
        "                                activation=None,\n",
        "                                batch_normalization=False)\n",
        "            # additions should now be executed without mis-dimension errors\n",
        "            x = keras.layers.add([x, b])\n",
        "        # change the current filter dimensions with the new one\n",
        "        no_filters = no_filters_next \n",
        "\n",
        "    # Finally the last stage which is Batch norm, relu followed by a polling layer\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    m = Flatten()(x) # convert to 1D \n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(m)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = builder_resnetv2(x_train.shape[1:], depth)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate_adjuster(0)),metrics=['accuracy'])\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "m_name = 'resnetv2_depth_%d_model.{epoch:03d}.h5' % depth\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, m_name)\n",
        "# callbacks for best saved models and for learning rates\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_acc',verbose=1,save_best_only=True)\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_adjuster)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),cooldown=0,patience=5,min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   448         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 32, 32, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 32, 32, 64)   0           add_13[0][0]                     \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 64)   256         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   4160        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 128)  8320        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 128)  0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 64)   8256        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 16, 16, 128)  0           add_15[0][0]                     \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 128)  512         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 128)    16512       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    147584      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 256)    33024       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 8, 8, 256)    0           conv2d_23[0][0]                  \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 256)    1024        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 128)    32896       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 128)    147584      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 256)    33024       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 8, 8, 256)    0           add_17[0][0]                     \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 256)    1024        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 256)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 574,090\n",
            "Trainable params: 570,602\n",
            "Non-trainable params: 3,488\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rCuK8mlXsu2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13654
        },
        "outputId": "9b4f4114-5bad-4abd-b66c-8c2c123fab81"
      },
      "cell_type": "code",
      "source": [
        "# Run training, with or without data augmentation.\n",
        "if retrain_resnet: \n",
        "    if not data_augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        history = model.fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  shuffle=True,\n",
        "                  callbacks=callbacks)\n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagenerator.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history = model.fit_generator(datagenerator.flow(x_train, y_train, batch_size=batch_size),\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            epochs=epochs, verbose=1, workers=4,\n",
        "                            callbacks=callbacks)\n",
        "        model = history.model\n",
        "        model.save_weights(save_dir+model_name_resnet)\n",
        "else:\n",
        "    model.load_weights(save_dir+model_name_resnet)\n",
        "    print ('Model weights loaded')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "196/196 [==============================] - 83s 422ms/step - loss: 1.8390 - acc: 0.4397 - val_loss: 2.0090 - val_acc: 0.4158\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.41580, saving model to /content/saved_models/resnetv2_depth_20_model.001.h5\n",
            "Epoch 2/200\n",
            "193/196 [============================>.] - ETA: 0s - loss: 1.4380 - acc: 0.5848"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 1.4364 - acc: 0.5853 - val_loss: 1.3824 - val_acc: 0.6041\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.41580 to 0.60410, saving model to /content/saved_models/resnetv2_depth_20_model.002.h5\n",
            "Epoch 3/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 1.2562 - acc: 0.6471 - val_loss: 1.3799 - val_acc: 0.6084\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.60410 to 0.60840, saving model to /content/saved_models/resnetv2_depth_20_model.003.h5\n",
            "Epoch 4/200\n",
            " 51/196 [======>.......................] - ETA: 39s - loss: 1.1750 - acc: 0.6708"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 1.1370 - acc: 0.6865 - val_loss: 1.4336 - val_acc: 0.5861\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.60840\n",
            "Epoch 5/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 1.0428 - acc: 0.7170 - val_loss: 1.1956 - val_acc: 0.6781\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.60840 to 0.67810, saving model to /content/saved_models/resnetv2_depth_20_model.005.h5\n",
            "Epoch 6/200\n",
            " 66/196 [=========>....................] - ETA: 35s - loss: 0.9801 - acc: 0.7384"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 59s 302ms/step - loss: 0.9719 - acc: 0.7405 - val_loss: 1.9609 - val_acc: 0.5517\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.67810\n",
            "Epoch 7/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.9091 - acc: 0.7652 - val_loss: 1.2481 - val_acc: 0.6625\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.67810\n",
            "Epoch 8/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.8715 - acc: 0.7768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.8652 - acc: 0.7783 - val_loss: 1.3040 - val_acc: 0.6686\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.67810\n",
            "Epoch 9/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.8303 - acc: 0.7892 - val_loss: 1.2753 - val_acc: 0.6633\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.67810\n",
            "Epoch 10/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.7883 - acc: 0.8035"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.7943 - acc: 0.8006 - val_loss: 0.9738 - val_acc: 0.7465\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.67810 to 0.74650, saving model to /content/saved_models/resnetv2_depth_20_model.010.h5\n",
            "Epoch 11/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.7653 - acc: 0.8098 - val_loss: 1.0559 - val_acc: 0.7295\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.74650\n",
            "Epoch 12/200\n",
            " 42/196 [=====>........................] - ETA: 42s - loss: 0.7329 - acc: 0.8239"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.7400 - acc: 0.8186 - val_loss: 0.9375 - val_acc: 0.7728\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.74650 to 0.77280, saving model to /content/saved_models/resnetv2_depth_20_model.012.h5\n",
            "Epoch 13/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.7202 - acc: 0.8222 - val_loss: 0.9287 - val_acc: 0.7614\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.77280\n",
            "Epoch 14/200\n",
            " 36/196 [====>.........................] - ETA: 43s - loss: 0.6915 - acc: 0.8363"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.7007 - acc: 0.8289 - val_loss: 0.9574 - val_acc: 0.7494\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.77280\n",
            "Epoch 15/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.6789 - acc: 0.8366 - val_loss: 0.8873 - val_acc: 0.7716\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.77280\n",
            "Epoch 16/200\n",
            " 74/196 [==========>...................] - ETA: 33s - loss: 0.6522 - acc: 0.8463"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.6594 - acc: 0.8432 - val_loss: 0.8137 - val_acc: 0.7939\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.77280 to 0.79390, saving model to /content/saved_models/resnetv2_depth_20_model.016.h5\n",
            "Epoch 17/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.6479 - acc: 0.8451 - val_loss: 0.8441 - val_acc: 0.7869\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.79390\n",
            "Epoch 18/200\n",
            " 41/196 [=====>........................] - ETA: 41s - loss: 0.6209 - acc: 0.8546"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.6287 - acc: 0.8528 - val_loss: 0.9477 - val_acc: 0.7536\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.79390\n",
            "Epoch 19/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.6161 - acc: 0.8568 - val_loss: 0.7646 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.79390 to 0.81010, saving model to /content/saved_models/resnetv2_depth_20_model.019.h5\n",
            "Epoch 20/200\n",
            " 63/196 [========>.....................] - ETA: 36s - loss: 0.6089 - acc: 0.8603"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.6021 - acc: 0.8619 - val_loss: 0.8010 - val_acc: 0.7978\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.81010\n",
            "Epoch 21/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5976 - acc: 0.8607 - val_loss: 0.8651 - val_acc: 0.7875\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.81010\n",
            "Epoch 22/200\n",
            " 79/196 [===========>..................] - ETA: 31s - loss: 0.5767 - acc: 0.8679"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5810 - acc: 0.8668 - val_loss: 0.7585 - val_acc: 0.8140\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.81010 to 0.81400, saving model to /content/saved_models/resnetv2_depth_20_model.022.h5\n",
            "Epoch 23/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.5773 - acc: 0.8675 - val_loss: 0.7667 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.81400\n",
            "Epoch 24/200\n",
            " 42/196 [=====>........................] - ETA: 42s - loss: 0.5395 - acc: 0.8832"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5626 - acc: 0.8728 - val_loss: 0.8056 - val_acc: 0.7974\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.81400\n",
            "Epoch 25/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5551 - acc: 0.8735 - val_loss: 0.7300 - val_acc: 0.8151\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.81400 to 0.81510, saving model to /content/saved_models/resnetv2_depth_20_model.025.h5\n",
            "Epoch 26/200\n",
            " 64/196 [========>.....................] - ETA: 36s - loss: 0.5319 - acc: 0.8832"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.5469 - acc: 0.8778 - val_loss: 0.7631 - val_acc: 0.8112\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.81510\n",
            "Epoch 27/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5380 - acc: 0.8791 - val_loss: 0.7221 - val_acc: 0.8224\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.81510 to 0.82240, saving model to /content/saved_models/resnetv2_depth_20_model.027.h5\n",
            "Epoch 28/200\n",
            " 67/196 [=========>....................] - ETA: 35s - loss: 0.5195 - acc: 0.8850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5280 - acc: 0.8830 - val_loss: 0.7499 - val_acc: 0.8156\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.82240\n",
            "Epoch 29/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.5173 - acc: 0.8861 - val_loss: 0.7256 - val_acc: 0.8257\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.82240 to 0.82570, saving model to /content/saved_models/resnetv2_depth_20_model.029.h5\n",
            "Epoch 30/200\n",
            " 67/196 [=========>....................] - ETA: 35s - loss: 0.5086 - acc: 0.8885"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.5128 - acc: 0.8868 - val_loss: 0.7593 - val_acc: 0.8212\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.82570\n",
            "Epoch 31/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.5125 - acc: 0.8872 - val_loss: 0.8462 - val_acc: 0.7982\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.82570\n",
            "Epoch 32/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.4966 - acc: 0.8918"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.5010 - acc: 0.8910 - val_loss: 0.6629 - val_acc: 0.8492\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.82570 to 0.84920, saving model to /content/saved_models/resnetv2_depth_20_model.032.h5\n",
            "Epoch 33/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.4950 - acc: 0.8944 - val_loss: 0.8861 - val_acc: 0.7976\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.84920\n",
            "Epoch 34/200\n",
            " 42/196 [=====>........................] - ETA: 41s - loss: 0.4747 - acc: 0.9022"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4882 - acc: 0.8962 - val_loss: 0.7242 - val_acc: 0.8230\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.84920\n",
            "Epoch 35/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4796 - acc: 0.8981 - val_loss: 0.8582 - val_acc: 0.7940\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.84920\n",
            "Epoch 36/200\n",
            " 75/196 [==========>...................] - ETA: 33s - loss: 0.4671 - acc: 0.9036"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4775 - acc: 0.8999 - val_loss: 0.8686 - val_acc: 0.7990\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.84920\n",
            "Epoch 37/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4741 - acc: 0.8990 - val_loss: 0.9677 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.84920\n",
            "Epoch 38/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.4569 - acc: 0.9043"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4651 - acc: 0.9017 - val_loss: 0.7297 - val_acc: 0.8271\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.84920\n",
            "Epoch 39/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4670 - acc: 0.9029 - val_loss: 0.6783 - val_acc: 0.8366\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.84920\n",
            "Epoch 40/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4551 - acc: 0.9073"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4604 - acc: 0.9043 - val_loss: 0.7343 - val_acc: 0.8258\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.84920\n",
            "Epoch 41/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.4551 - acc: 0.9058 - val_loss: 0.8154 - val_acc: 0.8147\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.84920\n",
            "Epoch 42/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4354 - acc: 0.9126"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4467 - acc: 0.9090 - val_loss: 0.8033 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.84920\n",
            "Epoch 43/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4439 - acc: 0.9091 - val_loss: 0.8855 - val_acc: 0.8006\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.84920\n",
            "Epoch 44/200\n",
            " 82/196 [===========>..................] - ETA: 30s - loss: 0.4314 - acc: 0.9130"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4442 - acc: 0.9091 - val_loss: 0.8817 - val_acc: 0.7932\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.84920\n",
            "Epoch 45/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4363 - acc: 0.9116 - val_loss: 0.7734 - val_acc: 0.8176\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.84920\n",
            "Epoch 46/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4258 - acc: 0.9151"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.4336 - acc: 0.9124 - val_loss: 0.7111 - val_acc: 0.8394\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.84920\n",
            "Epoch 47/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4265 - acc: 0.9152 - val_loss: 0.9792 - val_acc: 0.7765\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.84920\n",
            "Epoch 48/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4286 - acc: 0.9148"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.4332 - acc: 0.9128 - val_loss: 0.6670 - val_acc: 0.8443\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.84920\n",
            "Epoch 49/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4227 - acc: 0.9167 - val_loss: 0.9016 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.84920\n",
            "Epoch 50/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4168 - acc: 0.9195"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4253 - acc: 0.9162 - val_loss: 0.9221 - val_acc: 0.7929\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.84920\n",
            "Epoch 51/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4244 - acc: 0.9159 - val_loss: 0.7074 - val_acc: 0.8432\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.84920\n",
            "Epoch 52/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.4021 - acc: 0.9224"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4131 - acc: 0.9201 - val_loss: 0.7208 - val_acc: 0.8285\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.84920\n",
            "Epoch 53/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4102 - acc: 0.9195 - val_loss: 0.7971 - val_acc: 0.8187\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.84920\n",
            "Epoch 54/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3931 - acc: 0.9265"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.4117 - acc: 0.9190 - val_loss: 0.7986 - val_acc: 0.8247\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.84920\n",
            "Epoch 55/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.4065 - acc: 0.9214 - val_loss: 0.6135 - val_acc: 0.8574\n",
            "\n",
            "Epoch 00055: val_acc improved from 0.84920 to 0.85740, saving model to /content/saved_models/resnetv2_depth_20_model.055.h5\n",
            "Epoch 56/200\n",
            " 70/196 [=========>....................] - ETA: 34s - loss: 0.3969 - acc: 0.9249"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4046 - acc: 0.9227 - val_loss: 0.7600 - val_acc: 0.8302\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.85740\n",
            "Epoch 57/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4051 - acc: 0.9220 - val_loss: 0.7145 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.85740\n",
            "Epoch 58/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.3911 - acc: 0.9276"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.4012 - acc: 0.9238 - val_loss: 0.6662 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.85740\n",
            "Epoch 59/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.4026 - acc: 0.9244 - val_loss: 0.8557 - val_acc: 0.8170\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.85740\n",
            "Epoch 60/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3839 - acc: 0.9283"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3922 - acc: 0.9253 - val_loss: 0.7019 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.85740\n",
            "Epoch 61/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3892 - acc: 0.9276 - val_loss: 0.6809 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.85740\n",
            "Epoch 62/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3791 - acc: 0.9327"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3855 - acc: 0.9294 - val_loss: 0.8248 - val_acc: 0.8190\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.85740\n",
            "Epoch 63/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3886 - acc: 0.9286 - val_loss: 0.6564 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.85740\n",
            "Epoch 64/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3735 - acc: 0.9323"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3855 - acc: 0.9282 - val_loss: 0.7058 - val_acc: 0.8349\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.85740\n",
            "Epoch 65/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3855 - acc: 0.9287 - val_loss: 0.7321 - val_acc: 0.8479\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.85740\n",
            "Epoch 66/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3635 - acc: 0.9370"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3803 - acc: 0.9304 - val_loss: 0.8192 - val_acc: 0.8287\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.85740\n",
            "Epoch 67/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3851 - acc: 0.9292 - val_loss: 0.8965 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.85740\n",
            "Epoch 68/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3681 - acc: 0.9350"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3793 - acc: 0.9302 - val_loss: 1.1947 - val_acc: 0.7496\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.85740\n",
            "Epoch 69/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3784 - acc: 0.9313 - val_loss: 0.6291 - val_acc: 0.8645\n",
            "\n",
            "Epoch 00069: val_acc improved from 0.85740 to 0.86450, saving model to /content/saved_models/resnetv2_depth_20_model.069.h5\n",
            "Epoch 70/200\n",
            " 70/196 [=========>....................] - ETA: 34s - loss: 0.3603 - acc: 0.9375"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3705 - acc: 0.9341 - val_loss: 0.7013 - val_acc: 0.8382\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.86450\n",
            "Epoch 71/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3774 - acc: 0.9311 - val_loss: 0.7066 - val_acc: 0.8450\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.86450\n",
            "Epoch 72/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.3619 - acc: 0.9371"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3694 - acc: 0.9344 - val_loss: 0.6658 - val_acc: 0.8548\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.86450\n",
            "Epoch 73/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3724 - acc: 0.9327 - val_loss: 0.6819 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.86450\n",
            "Epoch 74/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3535 - acc: 0.9402"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3665 - acc: 0.9353 - val_loss: 0.7260 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.86450\n",
            "Epoch 75/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3643 - acc: 0.9357 - val_loss: 0.6603 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.86450\n",
            "Epoch 76/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3520 - acc: 0.9400"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3616 - acc: 0.9368 - val_loss: 0.7685 - val_acc: 0.8399\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.86450\n",
            "Epoch 77/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3666 - acc: 0.9353 - val_loss: 0.7511 - val_acc: 0.8395\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.86450\n",
            "Epoch 78/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3497 - acc: 0.9413"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3542 - acc: 0.9389 - val_loss: 0.8867 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.86450\n",
            "Epoch 79/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.3627 - acc: 0.9360 - val_loss: 0.7003 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.86450\n",
            "Epoch 80/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3499 - acc: 0.9414"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 59s 303ms/step - loss: 0.3594 - acc: 0.9382 - val_loss: 0.8244 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.86450\n",
            "Epoch 81/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.3608 - acc: 0.9373 - val_loss: 0.6816 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.86450\n",
            "Epoch 82/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.3127 - acc: 0.9563"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2986 - acc: 0.9614 - val_loss: 0.5137 - val_acc: 0.8987\n",
            "\n",
            "Epoch 00082: val_acc improved from 0.86450 to 0.89870, saving model to /content/saved_models/resnetv2_depth_20_model.082.h5\n",
            "Epoch 83/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2734 - acc: 0.9698 - val_loss: 0.5127 - val_acc: 0.8962\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.89870\n",
            "Epoch 84/200\n",
            " 42/196 [=====>........................] - ETA: 42s - loss: 0.2627 - acc: 0.9757"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2636 - acc: 0.9741 - val_loss: 0.5091 - val_acc: 0.8998\n",
            "\n",
            "Epoch 00084: val_acc improved from 0.89870 to 0.89980, saving model to /content/saved_models/resnetv2_depth_20_model.084.h5\n",
            "Epoch 85/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.2564 - acc: 0.9756 - val_loss: 0.5184 - val_acc: 0.8987\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.89980\n",
            "Epoch 86/200\n",
            " 36/196 [====>.........................] - ETA: 44s - loss: 0.2497 - acc: 0.9780"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2500 - acc: 0.9779 - val_loss: 0.5151 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.89980 to 0.90190, saving model to /content/saved_models/resnetv2_depth_20_model.086.h5\n",
            "Epoch 87/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2473 - acc: 0.9780 - val_loss: 0.5047 - val_acc: 0.9023\n",
            "\n",
            "Epoch 00087: val_acc improved from 0.90190 to 0.90230, saving model to /content/saved_models/resnetv2_depth_20_model.087.h5\n",
            "Epoch 88/200\n",
            " 30/196 [===>..........................] - ETA: 45s - loss: 0.2422 - acc: 0.9801"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2421 - acc: 0.9797 - val_loss: 0.5051 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00088: val_acc improved from 0.90230 to 0.90390, saving model to /content/saved_models/resnetv2_depth_20_model.088.h5\n",
            "Epoch 89/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2388 - acc: 0.9805 - val_loss: 0.5076 - val_acc: 0.9040\n",
            "\n",
            "Epoch 00089: val_acc improved from 0.90390 to 0.90400, saving model to /content/saved_models/resnetv2_depth_20_model.089.h5\n",
            "Epoch 90/200\n",
            " 29/196 [===>..........................] - ETA: 45s - loss: 0.2347 - acc: 0.9820"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2371 - acc: 0.9809 - val_loss: 0.5084 - val_acc: 0.9013\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.90400\n",
            "Epoch 91/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2315 - acc: 0.9822 - val_loss: 0.5071 - val_acc: 0.9053\n",
            "\n",
            "Epoch 00091: val_acc improved from 0.90400 to 0.90530, saving model to /content/saved_models/resnetv2_depth_20_model.091.h5\n",
            "Epoch 92/200\n",
            " 62/196 [========>.....................] - ETA: 37s - loss: 0.2259 - acc: 0.9847"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2271 - acc: 0.9836 - val_loss: 0.5057 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.90530\n",
            "Epoch 93/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2251 - acc: 0.9829 - val_loss: 0.5140 - val_acc: 0.9026\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.90530\n",
            "Epoch 94/200\n",
            " 79/196 [===========>..................] - ETA: 31s - loss: 0.2213 - acc: 0.9850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2217 - acc: 0.9846 - val_loss: 0.5154 - val_acc: 0.9038\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.90530\n",
            "Epoch 95/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2188 - acc: 0.9852 - val_loss: 0.5152 - val_acc: 0.9019\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.90530\n",
            "Epoch 96/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.2159 - acc: 0.9855"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2169 - acc: 0.9848 - val_loss: 0.5199 - val_acc: 0.9025\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.90530\n",
            "Epoch 97/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2116 - acc: 0.9870 - val_loss: 0.5199 - val_acc: 0.9046\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.90530\n",
            "Epoch 98/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.2107 - acc: 0.9866"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2113 - acc: 0.9861 - val_loss: 0.5294 - val_acc: 0.9030\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.90530\n",
            "Epoch 99/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2115 - acc: 0.9855 - val_loss: 0.5504 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.90530\n",
            "Epoch 100/200\n",
            " 82/196 [===========>..................] - ETA: 31s - loss: 0.2056 - acc: 0.9877"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2079 - acc: 0.9865 - val_loss: 0.5307 - val_acc: 0.9026\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.90530\n",
            "Epoch 101/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2062 - acc: 0.9866 - val_loss: 0.5255 - val_acc: 0.9028\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.90530\n",
            "Epoch 102/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.2042 - acc: 0.9862"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2047 - acc: 0.9866 - val_loss: 0.5241 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.90530\n",
            "Epoch 103/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.2004 - acc: 0.9885 - val_loss: 0.5211 - val_acc: 0.9033\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.90530\n",
            "Epoch 104/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1980 - acc: 0.9887"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.2003 - acc: 0.9879 - val_loss: 0.5254 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.90530\n",
            "Epoch 105/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1969 - acc: 0.9885 - val_loss: 0.5150 - val_acc: 0.9036\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.90530\n",
            "Epoch 106/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1953 - acc: 0.9891"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1963 - acc: 0.9884 - val_loss: 0.5283 - val_acc: 0.9029\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.90530\n",
            "Epoch 107/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1940 - acc: 0.9884 - val_loss: 0.5177 - val_acc: 0.9055\n",
            "\n",
            "Epoch 00107: val_acc improved from 0.90530 to 0.90550, saving model to /content/saved_models/resnetv2_depth_20_model.107.h5\n",
            "Epoch 108/200\n",
            " 69/196 [=========>....................] - ETA: 34s - loss: 0.1882 - acc: 0.9906"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1913 - acc: 0.9895 - val_loss: 0.5306 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.90550\n",
            "Epoch 109/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1905 - acc: 0.9893 - val_loss: 0.5418 - val_acc: 0.9002\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.90550\n",
            "Epoch 110/200\n",
            " 79/196 [===========>..................] - ETA: 32s - loss: 0.1872 - acc: 0.9910"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1892 - acc: 0.9897 - val_loss: 0.5277 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.90550\n",
            "Epoch 111/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1863 - acc: 0.9902 - val_loss: 0.5279 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.90550\n",
            "Epoch 112/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1843 - acc: 0.9901"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1845 - acc: 0.9901 - val_loss: 0.5402 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.90550\n",
            "Epoch 113/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1841 - acc: 0.9903 - val_loss: 0.5301 - val_acc: 0.9020\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.90550\n",
            "Epoch 114/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1815 - acc: 0.9913"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1813 - acc: 0.9909 - val_loss: 0.5212 - val_acc: 0.9034\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.90550\n",
            "Epoch 115/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1803 - acc: 0.9907 - val_loss: 0.5188 - val_acc: 0.9061\n",
            "\n",
            "Epoch 00115: val_acc improved from 0.90550 to 0.90610, saving model to /content/saved_models/resnetv2_depth_20_model.115.h5\n",
            "Epoch 116/200\n",
            " 69/196 [=========>....................] - ETA: 35s - loss: 0.1809 - acc: 0.9896"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1786 - acc: 0.9911 - val_loss: 0.5417 - val_acc: 0.8991\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.90610\n",
            "Epoch 117/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1776 - acc: 0.9905 - val_loss: 0.5255 - val_acc: 0.9039\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.90610\n",
            "Epoch 118/200\n",
            " 79/196 [===========>..................] - ETA: 32s - loss: 0.1741 - acc: 0.9926"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1764 - acc: 0.9911 - val_loss: 0.5402 - val_acc: 0.9004\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.90610\n",
            "Epoch 119/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1747 - acc: 0.9911 - val_loss: 0.5286 - val_acc: 0.9024\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.90610\n",
            "Epoch 120/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1736 - acc: 0.9907"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 298ms/step - loss: 0.1729 - acc: 0.9913 - val_loss: 0.5373 - val_acc: 0.8997\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.90610\n",
            "Epoch 121/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1714 - acc: 0.9921 - val_loss: 0.5326 - val_acc: 0.9045\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.90610\n",
            "Epoch 122/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1701 - acc: 0.9922"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1692 - acc: 0.9928 - val_loss: 0.5161 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00122: val_acc improved from 0.90610 to 0.90710, saving model to /content/saved_models/resnetv2_depth_20_model.122.h5\n",
            "Epoch 123/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1673 - acc: 0.9931 - val_loss: 0.5162 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00123: val_acc improved from 0.90710 to 0.90840, saving model to /content/saved_models/resnetv2_depth_20_model.123.h5\n",
            "Epoch 124/200\n",
            " 36/196 [====>.........................] - ETA: 44s - loss: 0.1689 - acc: 0.9920"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1656 - acc: 0.9941 - val_loss: 0.5163 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00124: val_acc improved from 0.90840 to 0.90870, saving model to /content/saved_models/resnetv2_depth_20_model.124.h5\n",
            "Epoch 125/200\n",
            "196/196 [==============================] - 58s 298ms/step - loss: 0.1666 - acc: 0.9932 - val_loss: 0.5158 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.90870\n",
            "Epoch 126/200\n",
            " 35/196 [====>.........................] - ETA: 43s - loss: 0.1666 - acc: 0.9924"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1652 - acc: 0.9940 - val_loss: 0.5141 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.90870\n",
            "Epoch 127/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1649 - acc: 0.9940 - val_loss: 0.5133 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.90870\n",
            "Epoch 128/200\n",
            " 73/196 [==========>...................] - ETA: 33s - loss: 0.1647 - acc: 0.9936"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1641 - acc: 0.9943 - val_loss: 0.5155 - val_acc: 0.9065\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.90870\n",
            "Epoch 129/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1639 - acc: 0.9949 - val_loss: 0.5156 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.90870\n",
            "Epoch 130/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.1651 - acc: 0.9939"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1639 - acc: 0.9943 - val_loss: 0.5149 - val_acc: 0.9078\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.90870\n",
            "Epoch 131/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1635 - acc: 0.9945 - val_loss: 0.5164 - val_acc: 0.9078\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.90870\n",
            "Epoch 132/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1629 - acc: 0.9945"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1633 - acc: 0.9942 - val_loss: 0.5147 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.90870\n",
            "Epoch 133/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1633 - acc: 0.9946 - val_loss: 0.5127 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.90870\n",
            "Epoch 134/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1631 - acc: 0.9939"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1634 - acc: 0.9942 - val_loss: 0.5133 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.90870\n",
            "Epoch 135/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1620 - acc: 0.9949 - val_loss: 0.5119 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.90870\n",
            "Epoch 136/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1612 - acc: 0.9951"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1619 - acc: 0.9949 - val_loss: 0.5169 - val_acc: 0.9076\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.90870\n",
            "Epoch 137/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1635 - acc: 0.9942 - val_loss: 0.5143 - val_acc: 0.9075\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.90870\n",
            "Epoch 138/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1623 - acc: 0.9945"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1628 - acc: 0.9942 - val_loss: 0.5160 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.90870\n",
            "Epoch 139/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1616 - acc: 0.9947 - val_loss: 0.5129 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.90870\n",
            "Epoch 140/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1621 - acc: 0.9946"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1620 - acc: 0.9947 - val_loss: 0.5154 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.90870\n",
            "Epoch 141/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1611 - acc: 0.9953 - val_loss: 0.5146 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.90870\n",
            "Epoch 142/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1606 - acc: 0.9952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.1607 - acc: 0.9955 - val_loss: 0.5167 - val_acc: 0.9079\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.90870\n",
            "Epoch 143/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1612 - acc: 0.9947 - val_loss: 0.5145 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.90870\n",
            "Epoch 144/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1599 - acc: 0.9954"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1603 - acc: 0.9952 - val_loss: 0.5186 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.90870\n",
            "Epoch 145/200\n",
            "196/196 [==============================] - 58s 297ms/step - loss: 0.1596 - acc: 0.9954 - val_loss: 0.5152 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.90870\n",
            "Epoch 146/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1602 - acc: 0.9951"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1607 - acc: 0.9948 - val_loss: 0.5193 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.90870\n",
            "Epoch 147/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1598 - acc: 0.9951 - val_loss: 0.5153 - val_acc: 0.9076\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.90870\n",
            "Epoch 148/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1592 - acc: 0.9954"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1595 - acc: 0.9952 - val_loss: 0.5153 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.90870\n",
            "Epoch 149/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1596 - acc: 0.9953 - val_loss: 0.5159 - val_acc: 0.9094\n",
            "\n",
            "Epoch 00149: val_acc improved from 0.90870 to 0.90940, saving model to /content/saved_models/resnetv2_depth_20_model.149.h5\n",
            "Epoch 150/200\n",
            " 69/196 [=========>....................] - ETA: 34s - loss: 0.1588 - acc: 0.9958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1593 - acc: 0.9954 - val_loss: 0.5157 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00150: val_acc improved from 0.90940 to 0.90970, saving model to /content/saved_models/resnetv2_depth_20_model.150.h5\n",
            "Epoch 151/200\n",
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1596 - acc: 0.9950 - val_loss: 0.5178 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.90970\n",
            "Epoch 152/200\n",
            " 40/196 [=====>........................] - ETA: 42s - loss: 0.1584 - acc: 0.9962"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 296ms/step - loss: 0.1596 - acc: 0.9951 - val_loss: 0.5160 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.90970\n",
            "Epoch 153/200\n",
            "196/196 [==============================] - 58s 295ms/step - loss: 0.1594 - acc: 0.9952 - val_loss: 0.5131 - val_acc: 0.9095\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.90970\n",
            "Epoch 154/200\n",
            " 74/196 [==========>...................] - ETA: 33s - loss: 0.1590 - acc: 0.9952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.1587 - acc: 0.9953 - val_loss: 0.5189 - val_acc: 0.9073\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.90970\n",
            "Epoch 155/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1587 - acc: 0.9954 - val_loss: 0.5192 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.90970\n",
            "Epoch 156/200\n",
            " 80/196 [===========>..................] - ETA: 31s - loss: 0.1603 - acc: 0.9946"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1594 - acc: 0.9949 - val_loss: 0.5195 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.90970\n",
            "Epoch 157/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1580 - acc: 0.9955 - val_loss: 0.5170 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.90970\n",
            "Epoch 158/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1583 - acc: 0.9955"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 293ms/step - loss: 0.1584 - acc: 0.9951 - val_loss: 0.5183 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.90970\n",
            "Epoch 159/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1567 - acc: 0.9961 - val_loss: 0.5211 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.90970\n",
            "Epoch 160/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1575 - acc: 0.9952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1577 - acc: 0.9953 - val_loss: 0.5216 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.90970\n",
            "Epoch 161/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1576 - acc: 0.9952 - val_loss: 0.5198 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.90970\n",
            "Epoch 162/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1566 - acc: 0.9951"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1567 - acc: 0.9954 - val_loss: 0.5197 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.90970\n",
            "Epoch 163/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1570 - acc: 0.9954 - val_loss: 0.5190 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.90970\n",
            "Epoch 164/200\n",
            " 82/196 [===========>..................] - ETA: 30s - loss: 0.1564 - acc: 0.9959"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 295ms/step - loss: 0.1567 - acc: 0.9958 - val_loss: 0.5188 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.90970\n",
            "Epoch 165/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1577 - acc: 0.9955 - val_loss: 0.5186 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.90970\n",
            "Epoch 166/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1586 - acc: 0.9948"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1568 - acc: 0.9957 - val_loss: 0.5186 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.90970\n",
            "Epoch 167/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1571 - acc: 0.9957 - val_loss: 0.5188 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.90970\n",
            "Epoch 168/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1569 - acc: 0.9960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1569 - acc: 0.9958 - val_loss: 0.5185 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.90970\n",
            "Epoch 169/200\n",
            "196/196 [==============================] - 57s 292ms/step - loss: 0.1570 - acc: 0.9958 - val_loss: 0.5184 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.90970\n",
            "Epoch 170/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1560 - acc: 0.9960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1566 - acc: 0.9957 - val_loss: 0.5176 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.90970\n",
            "Epoch 171/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1567 - acc: 0.9957 - val_loss: 0.5172 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.90970\n",
            "Epoch 172/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1580 - acc: 0.9948"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1577 - acc: 0.9953 - val_loss: 0.5181 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.90970\n",
            "Epoch 173/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1566 - acc: 0.9958 - val_loss: 0.5182 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.90970\n",
            "Epoch 174/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1561 - acc: 0.9957"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1567 - acc: 0.9957 - val_loss: 0.5184 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.90970\n",
            "Epoch 175/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1558 - acc: 0.9962 - val_loss: 0.5184 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.90970\n",
            "Epoch 176/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1554 - acc: 0.9962"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1563 - acc: 0.9955 - val_loss: 0.5187 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.90970\n",
            "Epoch 177/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1569 - acc: 0.9956 - val_loss: 0.5183 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.90970\n",
            "Epoch 178/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1555 - acc: 0.9965"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1566 - acc: 0.9957 - val_loss: 0.5187 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.90970\n",
            "Epoch 179/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1561 - acc: 0.9958 - val_loss: 0.5184 - val_acc: 0.9079\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.90970\n",
            "Epoch 180/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1559 - acc: 0.9958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1563 - acc: 0.9957 - val_loss: 0.5187 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.90970\n",
            "Epoch 181/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1569 - acc: 0.9957 - val_loss: 0.5190 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.90970\n",
            "Epoch 182/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1561 - acc: 0.9959"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1561 - acc: 0.9958 - val_loss: 0.5190 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.90970\n",
            "Epoch 183/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1570 - acc: 0.9955 - val_loss: 0.5190 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.90970\n",
            "Epoch 184/200\n",
            " 82/196 [===========>..................] - ETA: 30s - loss: 0.1579 - acc: 0.9952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1567 - acc: 0.9957 - val_loss: 0.5183 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.90970\n",
            "Epoch 185/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1570 - acc: 0.9954 - val_loss: 0.5196 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.90970\n",
            "Epoch 186/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1557 - acc: 0.9958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1564 - acc: 0.9957 - val_loss: 0.5195 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.90970\n",
            "Epoch 187/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1559 - acc: 0.9959 - val_loss: 0.5193 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.90970\n",
            "Epoch 188/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1575 - acc: 0.9953"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1568 - acc: 0.9957 - val_loss: 0.5193 - val_acc: 0.9085\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.90970\n",
            "Epoch 189/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1563 - acc: 0.9957 - val_loss: 0.5190 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.90970\n",
            "Epoch 190/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1560 - acc: 0.9956"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1563 - acc: 0.9957 - val_loss: 0.5198 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.90970\n",
            "Epoch 191/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1559 - acc: 0.9959 - val_loss: 0.5187 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.90970\n",
            "Epoch 192/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1557 - acc: 0.9960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1561 - acc: 0.9957 - val_loss: 0.5193 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.90970\n",
            "Epoch 193/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1565 - acc: 0.9957 - val_loss: 0.5198 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.90970\n",
            "Epoch 194/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1555 - acc: 0.9960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 293ms/step - loss: 0.1557 - acc: 0.9960 - val_loss: 0.5189 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.90970\n",
            "Epoch 195/200\n",
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1562 - acc: 0.9961 - val_loss: 0.5196 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.90970\n",
            "Epoch 196/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1568 - acc: 0.9958"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 293ms/step - loss: 0.1564 - acc: 0.9958 - val_loss: 0.5190 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.90970\n",
            "Epoch 197/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1568 - acc: 0.9957 - val_loss: 0.5194 - val_acc: 0.9083\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.90970\n",
            "Epoch 198/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1566 - acc: 0.9952"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 293ms/step - loss: 0.1572 - acc: 0.9951 - val_loss: 0.5191 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.90970\n",
            "Epoch 199/200\n",
            "196/196 [==============================] - 57s 293ms/step - loss: 0.1561 - acc: 0.9958 - val_loss: 0.5188 - val_acc: 0.9080\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.90970\n",
            "Epoch 200/200\n",
            " 81/196 [===========>..................] - ETA: 31s - loss: 0.1563 - acc: 0.9951"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 58s 294ms/step - loss: 0.1567 - acc: 0.9951 - val_loss: 0.5198 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.90970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ov5t6BRs9M0c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results "
      ]
    },
    {
      "metadata": {
        "id": "YMK5rdwAuOSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9ccf39d9-6436-4f0e-f622-ad6886ba0dd9"
      },
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 8s 767us/step\n",
            "Test loss: 0.5198415959835052\n",
            "Test accuracy: 0.9081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aM6_NFWSuPrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "176828cd-06bf-4681-9648-8232c5ae500b"
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "fig = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(histry.history['acc'])\n",
        "plt.plot(histry.history['val_acc'])\n",
        "plt.axhline(y=0.5, color='grey', linestyle='--')\n",
        "#plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "# summarize history for loss\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(histry.history['loss'])\n",
        "plt.plot(histry.history['val_loss'])\n",
        "plt.axhline(y=0.693, color='grey', linestyle='--')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig.savefig('learning_curve_Resnetv2.png', bbox_inches='tight')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRpK3EEII0c/8f5H0MDyrhp2YAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5c462c7a90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FXJJmoaPukS0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Observation and Discussion \n",
        "\n",
        "In this section I showed a ResnetV2 implementation and I implemented it as shown in the paper [1]. A data augemntation generator was used to randomly shift and flip the images and add some whiting noise. The Resnet architecture is of 20 layers deep of total number of parmeter about 500K+. Using the GPU on this machine, It took about 40 seconds on average for every epoch. 200 Epoches were run and the best run achieved 90.8 % accuracy on the validation set. It is worth to mention that I ran the same algorithim on my 4 quad mahcine (no GPU) which took about 25 hours on 120 epoches to run and it achieved 92% accuracy. I guess this 1 percent discripency is due to the random weights intialized so a seed setup should be applied to make the results more replicatable. It looks like deep arcitectures in general achieves a good job than the simple conv net. The next sectoin will discribe my research path from here. We can either get deeper with the archtecture or think of ways to optimize the model. "
      ]
    },
    {
      "metadata": {
        "id": "WKsec81hFtuL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now Whats Next ?\n",
        "\n",
        "There are plenty of aproaches to try but the time and resources are limted so let us take a step back and observe what was done so far and decide what are our next steps for this short study.  From the previous sections we showed a very complicated architecture such as resnet that gave fairly good results and also a very simple model that also showed reletively good results. The resnet-20 has 574,090 parameters and the convnet has much fewers parmas to train. Even though resnet showed better results but the comparison between performances is not fair. Both approaches were run on mean subtracted and pixel normalized data but the convolution net approach did not run using a learning rate scheduler, and the data agmentation generator was not applied too. I do not think it will surpase the performance of the resnet but it should improve the performance of the conv net approach. For the sake of completion I shall do that as my next step. Generaly, research papers show [1],[2],[3] that the deeper the model the better the results of the image classifcaitons. This is true but does that mean that simplier architectures are bad and we should not use them at all ? Also How much difference of accuracy performance are we gaining from deeper models and what is the time complexity trade off ? \n",
        "\n",
        "Now what I can try next can either be a more deeper architecture and try to reach to 95% accuracy by using a ResnetV2 of 1001 in depth or any other deep and complex architecture, then train it for like 5 days so I can only gain 3 %  -5 % extra accuracy or 27 hours using 2 GPUs [1]. On the other hand, we can try different approaches and try to reach to similar results with much less computational complexity trading off little accuracy perfromance. Neither approaches are wrong or right but it depends on the business problem or the application we have in hand. Since this is just a take home exam, I will try to use simpler models of different architectures to explore diversity.\n",
        "\n",
        "The previous models are based on the traditional way of computing convolution but there are other reasnably deep approaches that uses what is called Deepth Seprable Convolution. This will perform a spatial convolution while keeping the channels (3 channels in case of RBG images) separate and then follow with a depthwise convolution. For more information about how depthwise convolutin work please have a look at [4]. The advantage of using deepth seprable convolution is that It cuts the number of multiplications by a factor of 9 in a convolution net compared to the standard convolution with a little comprimise on accuracy as shown in mobileNet architecture. \n",
        "\n",
        "There are lots of models I would love to try but I think I will choose the Xception architecture, a convolutional neural network architecture based entirely on depthwise separable convolution layers. The Xception architecture has 36 convolutional layers forming the feature extraction base of the network.  The 36 convolutional layers are structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules. In short, the Xception architecture is a linear stack of depthwise separable convolution layers with residual skip connection [5]. It is inspired by the inception model and basically an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions.\n",
        "\n",
        "In the next sections I will redo the simple convolution approach one more time with data agmentation and a call back function. Also I will show the results of the xception model using a keras implementation. \n"
      ]
    },
    {
      "metadata": {
        "id": "KyIZAlfrmFZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ConvNet with Data Agumentation and Adaptable Learning Rate: \n",
        "\n",
        "The following code implements a similar simple conv net architecture to the one I tried before. For the sake of completness I wanted to run the same model using data augmentation and see if there are any changes in the results. Even though all the papers agrees that deep architectures achieves higher accuracy but I wanted to quantify how much improvements they mean by that, also its is worth to mention that most of the papers out there use the image net data set as a bench mark which is a different dataset used in this study. \n",
        "\n",
        "Things I tried that is not in the following code: \n",
        " \n",
        " - If you notice the difference in architecture between this implementation and the previous one, you will notice that I added an extra layer of 128 filter size and an extra dropout. The result of both archiectures are very similar only 0.5% difference so it didnt really create a considrable significant change. \n",
        " \n",
        " - I also tried taking z transform (mean subtraction then divide by standard diviation) to normalize the data to unit distribution but again I did not notice a big change in the results either. \n"
      ]
    },
    {
      "metadata": {
        "id": "I-dYFL2BHbjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "n-EoK3S1Ba_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b67e9c0-1179-4513-d71c-5a828e0ca54f"
      },
      "cell_type": "code",
      "source": [
        "# retrain model or use saved model\n",
        "retrain_convnet_augmentation = True\n",
        "#save model_after_train \n",
        "model_name_conv_agmentation = 'convnet_augmentation_trained.h5'\n",
        "# Data augmentation \n",
        "data_augmentation = True\n",
        "# Training parameters\n",
        "batch_size = 256\n",
        "epochs = 200\n",
        "\n",
        "# Reload data again this time try z-score instead of regular way of normalization\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train =  keras.utils.to_categorical(y_train,num_classes)\n",
        "y_test =  keras.utils.to_categorical(y_test,num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 34s 0us/step\n",
            "170508288/170498071 [==============================] - 34s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RUVjRiEKBruA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "24f3a0b4-33c0-4532-b2fc-66e945c24c4a"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(1e-4), input_shape=x_train.shape[1:]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3, 3),padding='same', kernel_regularizer=l2(1e-4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4)))  \n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(1e-4)))  \n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(1e-4)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms,metrics=['accuracy'])\n",
        "\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "m_name2 = 'convnet_agmentation_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, m_name2)\n",
        "# callbacks for best saved models and for learning rates\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_acc',verbose=1,save_best_only=True)\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_adjuster)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),cooldown=0,patience=5,min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_35 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BctORKccGSlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13637
        },
        "outputId": "36283a92-f8ae-4656-c1e0-fd4d04cd7732"
      },
      "cell_type": "code",
      "source": [
        "if retrain_convnet_augmentation: \n",
        "  histry = model.fit_generator(datagenerator.flow(x_train, y_train, batch_size=batch_size),\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            epochs=epochs, verbose=1,\n",
        "                            callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "  model = histry.model\n",
        "  model.save_weights(save_dir+model_name_conv_agmentation)\n",
        "else:\n",
        "  model.load_weights(save_dir+model_name_conv_agmentation)\n",
        "  print ('Model weights loaded')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "196/196 [==============================] - 44s 224ms/step - loss: 1.9714 - acc: 0.3674 - val_loss: 1.6754 - val_acc: 0.4845\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.48450, saving model to /content/saved_models/convnet_agmentation_20_model.001.h5\n",
            "Epoch 2/200\n",
            "196/196 [==============================] - 36s 184ms/step - loss: 1.4971 - acc: 0.4973 - val_loss: 1.3510 - val_acc: 0.5894\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.48450 to 0.58940, saving model to /content/saved_models/convnet_agmentation_20_model.002.h5\n",
            "Epoch 3/200\n",
            " 17/196 [=>............................] - ETA: 25s - loss: 1.3625 - acc: 0.5377"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 1.3096 - acc: 0.5616 - val_loss: 1.1570 - val_acc: 0.6443\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.58940 to 0.64430, saving model to /content/saved_models/convnet_agmentation_20_model.003.h5\n",
            "Epoch 4/200\n",
            "196/196 [==============================] - 36s 185ms/step - loss: 1.2208 - acc: 0.5894 - val_loss: 1.1776 - val_acc: 0.6380\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.64430\n",
            "Epoch 5/200\n",
            " 32/196 [===>..........................] - ETA: 25s - loss: 1.1916 - acc: 0.6040"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 186ms/step - loss: 1.1656 - acc: 0.6127 - val_loss: 1.1108 - val_acc: 0.6476\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.64430 to 0.64760, saving model to /content/saved_models/convnet_agmentation_20_model.005.h5\n",
            "Epoch 6/200\n",
            "196/196 [==============================] - 36s 185ms/step - loss: 1.1167 - acc: 0.6302 - val_loss: 1.0616 - val_acc: 0.6651\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.64760 to 0.66510, saving model to /content/saved_models/convnet_agmentation_20_model.006.h5\n",
            "Epoch 7/200\n",
            " 28/196 [===>..........................] - ETA: 24s - loss: 1.0877 - acc: 0.6449"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 186ms/step - loss: 1.0893 - acc: 0.6401 - val_loss: 0.9732 - val_acc: 0.6893\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.66510 to 0.68930, saving model to /content/saved_models/convnet_agmentation_20_model.007.h5\n",
            "Epoch 8/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 1.0611 - acc: 0.6516 - val_loss: 0.9290 - val_acc: 0.7076\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.68930 to 0.70760, saving model to /content/saved_models/convnet_agmentation_20_model.008.h5\n",
            "Epoch 9/200\n",
            " 27/196 [===>..........................] - ETA: 25s - loss: 1.0247 - acc: 0.6569"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 1.0394 - acc: 0.6580 - val_loss: 1.0440 - val_acc: 0.6751\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.70760\n",
            "Epoch 10/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 1.0250 - acc: 0.6664 - val_loss: 0.8664 - val_acc: 0.7287\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.70760 to 0.72870, saving model to /content/saved_models/convnet_agmentation_20_model.010.h5\n",
            "Epoch 11/200\n",
            " 58/196 [=======>......................] - ETA: 23s - loss: 1.0138 - acc: 0.6742"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 185ms/step - loss: 1.0079 - acc: 0.6742 - val_loss: 0.9520 - val_acc: 0.7082\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.72870\n",
            "Epoch 12/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.9899 - acc: 0.6789 - val_loss: 0.8598 - val_acc: 0.7346\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.72870 to 0.73460, saving model to /content/saved_models/convnet_agmentation_20_model.012.h5\n",
            "Epoch 13/200\n",
            " 66/196 [=========>....................] - ETA: 22s - loss: 0.9851 - acc: 0.6827"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 185ms/step - loss: 0.9830 - acc: 0.6832 - val_loss: 0.8987 - val_acc: 0.7222\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.73460\n",
            "Epoch 14/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.9731 - acc: 0.6852 - val_loss: 0.9360 - val_acc: 0.7108\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.73460\n",
            "Epoch 15/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.9575 - acc: 0.6922"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.9601 - acc: 0.6922 - val_loss: 0.8635 - val_acc: 0.7316\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.73460\n",
            "Epoch 16/200\n",
            "196/196 [==============================] - 37s 189ms/step - loss: 0.9482 - acc: 0.6950 - val_loss: 0.8584 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.73460\n",
            "Epoch 17/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.9427 - acc: 0.6944"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 191ms/step - loss: 0.9406 - acc: 0.6965 - val_loss: 0.9051 - val_acc: 0.7245\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.73460\n",
            "Epoch 18/200\n",
            "196/196 [==============================] - 37s 189ms/step - loss: 0.9356 - acc: 0.7018 - val_loss: 0.8488 - val_acc: 0.7424\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.73460 to 0.74240, saving model to /content/saved_models/convnet_agmentation_20_model.018.h5\n",
            "Epoch 19/200\n",
            " 67/196 [=========>....................] - ETA: 22s - loss: 0.9194 - acc: 0.7055"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.9296 - acc: 0.7031 - val_loss: 0.8119 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.74240 to 0.75000, saving model to /content/saved_models/convnet_agmentation_20_model.019.h5\n",
            "Epoch 20/200\n",
            "196/196 [==============================] - 38s 194ms/step - loss: 0.9313 - acc: 0.7039 - val_loss: 0.8396 - val_acc: 0.7397\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.75000\n",
            "Epoch 21/200\n",
            " 36/196 [====>.........................] - ETA: 25s - loss: 0.9217 - acc: 0.7033"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.9154 - acc: 0.7085 - val_loss: 0.8067 - val_acc: 0.7581\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.75000 to 0.75810, saving model to /content/saved_models/convnet_agmentation_20_model.021.h5\n",
            "Epoch 22/200\n",
            "196/196 [==============================] - 36s 186ms/step - loss: 0.9178 - acc: 0.7092 - val_loss: 0.9743 - val_acc: 0.7038\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.75810\n",
            "Epoch 23/200\n",
            " 34/196 [====>.........................] - ETA: 26s - loss: 0.8879 - acc: 0.7151"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.9042 - acc: 0.7111 - val_loss: 0.8744 - val_acc: 0.7346\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.75810\n",
            "Epoch 24/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.9028 - acc: 0.7154 - val_loss: 0.9206 - val_acc: 0.7189\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.75810\n",
            "Epoch 25/200\n",
            " 73/196 [==========>...................] - ETA: 21s - loss: 0.9053 - acc: 0.7124"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8992 - acc: 0.7154 - val_loss: 0.8003 - val_acc: 0.7580\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.75810\n",
            "Epoch 26/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.9018 - acc: 0.7141 - val_loss: 0.7749 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.75810 to 0.76690, saving model to /content/saved_models/convnet_agmentation_20_model.026.h5\n",
            "Epoch 27/200\n",
            " 67/196 [=========>....................] - ETA: 21s - loss: 0.8883 - acc: 0.7211"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8965 - acc: 0.7174 - val_loss: 0.8379 - val_acc: 0.7429\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.76690\n",
            "Epoch 28/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8896 - acc: 0.7213 - val_loss: 0.7814 - val_acc: 0.7633\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.76690\n",
            "Epoch 29/200\n",
            " 76/196 [==========>...................] - ETA: 20s - loss: 0.8810 - acc: 0.7191"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8929 - acc: 0.7173 - val_loss: 0.8122 - val_acc: 0.7551\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.76690\n",
            "Epoch 30/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8859 - acc: 0.7234 - val_loss: 0.9108 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.76690\n",
            "Epoch 31/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.8856 - acc: 0.7239"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 186ms/step - loss: 0.8883 - acc: 0.7234 - val_loss: 0.8272 - val_acc: 0.7494\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.76690\n",
            "Epoch 32/200\n",
            "196/196 [==============================] - 36s 185ms/step - loss: 0.8799 - acc: 0.7245 - val_loss: 0.8644 - val_acc: 0.7442\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.76690\n",
            "Epoch 33/200\n",
            " 82/196 [===========>..................] - ETA: 19s - loss: 0.8832 - acc: 0.7262"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.8826 - acc: 0.7247 - val_loss: 0.8774 - val_acc: 0.7425\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.76690\n",
            "Epoch 34/200\n",
            "196/196 [==============================] - 36s 186ms/step - loss: 0.8828 - acc: 0.7255 - val_loss: 0.8794 - val_acc: 0.7378\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.76690\n",
            "Epoch 35/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8790 - acc: 0.7259"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 186ms/step - loss: 0.8807 - acc: 0.7260 - val_loss: 0.8366 - val_acc: 0.7481\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.76690\n",
            "Epoch 36/200\n",
            "196/196 [==============================] - 36s 185ms/step - loss: 0.8740 - acc: 0.7268 - val_loss: 0.8137 - val_acc: 0.7564\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.76690\n",
            "Epoch 37/200\n",
            " 82/196 [===========>..................] - ETA: 19s - loss: 0.8739 - acc: 0.7294"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8747 - acc: 0.7271 - val_loss: 0.7445 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.76690 to 0.77640, saving model to /content/saved_models/convnet_agmentation_20_model.037.h5\n",
            "Epoch 38/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8704 - acc: 0.7292 - val_loss: 0.7948 - val_acc: 0.7650\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.77640\n",
            "Epoch 39/200\n",
            " 39/196 [====>.........................] - ETA: 25s - loss: 0.8721 - acc: 0.7284"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8693 - acc: 0.7306 - val_loss: 0.7809 - val_acc: 0.7676\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.77640\n",
            "Epoch 40/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8668 - acc: 0.7320 - val_loss: 0.8422 - val_acc: 0.7470\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.77640\n",
            "Epoch 41/200\n",
            " 67/196 [=========>....................] - ETA: 22s - loss: 0.8660 - acc: 0.7315"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8681 - acc: 0.7322 - val_loss: 0.7749 - val_acc: 0.7748\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.77640\n",
            "Epoch 42/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8694 - acc: 0.7315 - val_loss: 0.7745 - val_acc: 0.7722\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.77640\n",
            "Epoch 43/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.8514 - acc: 0.7381"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8605 - acc: 0.7330 - val_loss: 0.7590 - val_acc: 0.7798\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.77640 to 0.77980, saving model to /content/saved_models/convnet_agmentation_20_model.043.h5\n",
            "Epoch 44/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8647 - acc: 0.7336 - val_loss: 0.8815 - val_acc: 0.7483\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.77980\n",
            "Epoch 45/200\n",
            " 39/196 [====>.........................] - ETA: 25s - loss: 0.8525 - acc: 0.7389"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8680 - acc: 0.7335 - val_loss: 0.7867 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.77980\n",
            "Epoch 46/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8607 - acc: 0.7332 - val_loss: 0.7714 - val_acc: 0.7717\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.77980\n",
            "Epoch 47/200\n",
            " 74/196 [==========>...................] - ETA: 21s - loss: 0.8514 - acc: 0.7372"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8598 - acc: 0.7352 - val_loss: 0.8822 - val_acc: 0.7399\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.77980\n",
            "Epoch 48/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8604 - acc: 0.7323 - val_loss: 0.7771 - val_acc: 0.7729\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.77980\n",
            "Epoch 49/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.8598 - acc: 0.7373\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8581 - acc: 0.7385 - val_loss: 0.7338 - val_acc: 0.7853\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.77980 to 0.78530, saving model to /content/saved_models/convnet_agmentation_20_model.049.h5\n",
            "Epoch 50/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8498 - acc: 0.7380 - val_loss: 0.7605 - val_acc: 0.7737\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.78530\n",
            "Epoch 51/200\n",
            " 40/196 [=====>........................] - ETA: 25s - loss: 0.8367 - acc: 0.7480"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8597 - acc: 0.7371 - val_loss: 0.8091 - val_acc: 0.7608\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.78530\n",
            "Epoch 52/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8542 - acc: 0.7391 - val_loss: 0.8562 - val_acc: 0.7496\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.78530\n",
            "Epoch 53/200\n",
            " 75/196 [==========>...................] - ETA: 20s - loss: 0.8433 - acc: 0.7417"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8527 - acc: 0.7382 - val_loss: 0.7474 - val_acc: 0.7842\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.78530\n",
            "Epoch 54/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8542 - acc: 0.7383 - val_loss: 0.7737 - val_acc: 0.7752\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.78530\n",
            "Epoch 55/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8427 - acc: 0.7423"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8499 - acc: 0.7392 - val_loss: 0.7416 - val_acc: 0.7822\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.78530\n",
            "Epoch 56/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8489 - acc: 0.7393 - val_loss: 0.8083 - val_acc: 0.7586\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.78530\n",
            "Epoch 57/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.8449 - acc: 0.7408"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.8497 - acc: 0.7396 - val_loss: 0.8131 - val_acc: 0.7653\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.78530\n",
            "Epoch 58/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8402 - acc: 0.7423 - val_loss: 0.7373 - val_acc: 0.7784\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.78530\n",
            "Epoch 59/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.8430 - acc: 0.7413"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8500 - acc: 0.7396 - val_loss: 0.8145 - val_acc: 0.7622\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.78530\n",
            "Epoch 60/200\n",
            "196/196 [==============================] - 37s 186ms/step - loss: 0.8456 - acc: 0.7410 - val_loss: 0.7883 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.78530\n",
            "Epoch 61/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8441 - acc: 0.7395"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8447 - acc: 0.7405 - val_loss: 0.7666 - val_acc: 0.7752\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.78530\n",
            "Epoch 62/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8437 - acc: 0.7426 - val_loss: 0.7419 - val_acc: 0.7825\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.78530\n",
            "Epoch 63/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8441 - acc: 0.7386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8458 - acc: 0.7393 - val_loss: 0.8602 - val_acc: 0.7425\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.78530\n",
            "Epoch 64/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8375 - acc: 0.7413 - val_loss: 0.7531 - val_acc: 0.7818\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.78530\n",
            "Epoch 65/200\n",
            " 83/196 [===========>..................] - ETA: 19s - loss: 0.8384 - acc: 0.7468"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8387 - acc: 0.7453 - val_loss: 0.7401 - val_acc: 0.7820\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.78530\n",
            "Epoch 66/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8389 - acc: 0.7446 - val_loss: 0.7846 - val_acc: 0.7669\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.78530\n",
            "Epoch 67/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.8398 - acc: 0.7440"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8421 - acc: 0.7428 - val_loss: 0.7707 - val_acc: 0.7771\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.78530\n",
            "Epoch 68/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8393 - acc: 0.7455 - val_loss: 0.7532 - val_acc: 0.7769\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.78530\n",
            "Epoch 69/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8376 - acc: 0.7422"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8424 - acc: 0.7429 - val_loss: 0.7785 - val_acc: 0.7730\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.78530\n",
            "Epoch 70/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8371 - acc: 0.7470 - val_loss: 0.7812 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.78530\n",
            "Epoch 71/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8298 - acc: 0.7460"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8421 - acc: 0.7422 - val_loss: 0.7091 - val_acc: 0.7933\n",
            "\n",
            "Epoch 00071: val_acc improved from 0.78530 to 0.79330, saving model to /content/saved_models/convnet_agmentation_20_model.071.h5\n",
            "Epoch 72/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8348 - acc: 0.7465 - val_loss: 0.7550 - val_acc: 0.7798\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.79330\n",
            "Epoch 73/200\n",
            " 40/196 [=====>........................] - ETA: 25s - loss: 0.8458 - acc: 0.7382"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8407 - acc: 0.7426 - val_loss: 0.7340 - val_acc: 0.7881\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.79330\n",
            "Epoch 74/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8396 - acc: 0.7451 - val_loss: 0.7741 - val_acc: 0.7769\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.79330\n",
            "Epoch 75/200\n",
            " 69/196 [=========>....................] - ETA: 22s - loss: 0.8372 - acc: 0.7462"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8347 - acc: 0.7451 - val_loss: 0.7119 - val_acc: 0.7886\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.79330\n",
            "Epoch 76/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.8403 - acc: 0.7462 - val_loss: 0.8542 - val_acc: 0.7517\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.79330\n",
            "Epoch 77/200\n",
            " 81/196 [===========>..................] - ETA: 19s - loss: 0.8372 - acc: 0.7471"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8340 - acc: 0.7465 - val_loss: 0.8401 - val_acc: 0.7568\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.79330\n",
            "Epoch 78/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8298 - acc: 0.7465 - val_loss: 0.7518 - val_acc: 0.7787\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.79330\n",
            "Epoch 79/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.8306 - acc: 0.7513"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8338 - acc: 0.7477 - val_loss: 0.7946 - val_acc: 0.7668\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.79330\n",
            "Epoch 80/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8296 - acc: 0.7492 - val_loss: 0.7509 - val_acc: 0.7827\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.79330\n",
            "Epoch 81/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.8262 - acc: 0.7472"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.8347 - acc: 0.7447 - val_loss: 0.7239 - val_acc: 0.7819\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.79330\n",
            "Epoch 82/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7918 - acc: 0.7608 - val_loss: 0.7047 - val_acc: 0.7940\n",
            "\n",
            "Epoch 00082: val_acc improved from 0.79330 to 0.79400, saving model to /content/saved_models/convnet_agmentation_20_model.082.h5\n",
            "Epoch 83/200\n",
            " 66/196 [=========>....................] - ETA: 22s - loss: 0.7709 - acc: 0.7680"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7741 - acc: 0.7664 - val_loss: 0.6935 - val_acc: 0.7978\n",
            "\n",
            "Epoch 00083: val_acc improved from 0.79400 to 0.79780, saving model to /content/saved_models/convnet_agmentation_20_model.083.h5\n",
            "Epoch 84/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7682 - acc: 0.7697 - val_loss: 0.7066 - val_acc: 0.7944\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.79780\n",
            "Epoch 85/200\n",
            " 34/196 [====>.........................] - ETA: 26s - loss: 0.7711 - acc: 0.7678"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7638 - acc: 0.7695 - val_loss: 0.6855 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.79780 to 0.80000, saving model to /content/saved_models/convnet_agmentation_20_model.085.h5\n",
            "Epoch 86/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7647 - acc: 0.7696 - val_loss: 0.6714 - val_acc: 0.8013\n",
            "\n",
            "Epoch 00086: val_acc improved from 0.80000 to 0.80130, saving model to /content/saved_models/convnet_agmentation_20_model.086.h5\n",
            "Epoch 87/200\n",
            " 25/196 [==>...........................] - ETA: 24s - loss: 0.7281 - acc: 0.7832"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7578 - acc: 0.7705 - val_loss: 0.6945 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.80130\n",
            "Epoch 88/200\n",
            "196/196 [==============================] - 36s 186ms/step - loss: 0.7584 - acc: 0.7713 - val_loss: 0.6879 - val_acc: 0.7976\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.80130\n",
            "Epoch 89/200\n",
            " 71/196 [=========>....................] - ETA: 21s - loss: 0.7565 - acc: 0.7710"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7531 - acc: 0.7727 - val_loss: 0.6822 - val_acc: 0.8007\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.80130\n",
            "Epoch 90/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7541 - acc: 0.7720 - val_loss: 0.6752 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00090: val_acc improved from 0.80130 to 0.80400, saving model to /content/saved_models/convnet_agmentation_20_model.090.h5\n",
            "Epoch 91/200\n",
            " 67/196 [=========>....................] - ETA: 21s - loss: 0.7457 - acc: 0.7748"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7495 - acc: 0.7746 - val_loss: 0.6729 - val_acc: 0.8017\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.80400\n",
            "Epoch 92/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7471 - acc: 0.7748 - val_loss: 0.6844 - val_acc: 0.8015\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.80400\n",
            "Epoch 93/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7483 - acc: 0.7736"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7442 - acc: 0.7759 - val_loss: 0.6764 - val_acc: 0.8029\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.80400\n",
            "Epoch 94/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7411 - acc: 0.7766 - val_loss: 0.6574 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00094: val_acc improved from 0.80400 to 0.80740, saving model to /content/saved_models/convnet_agmentation_20_model.094.h5\n",
            "Epoch 95/200\n",
            " 65/196 [========>.....................] - ETA: 22s - loss: 0.7572 - acc: 0.7718"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7398 - acc: 0.7772 - val_loss: 0.6797 - val_acc: 0.8038\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.80740\n",
            "Epoch 96/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7423 - acc: 0.7753 - val_loss: 0.6750 - val_acc: 0.8019\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.80740\n",
            "Epoch 97/200\n",
            " 74/196 [==========>...................] - ETA: 21s - loss: 0.7540 - acc: 0.7719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7453 - acc: 0.7752 - val_loss: 0.6554 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.80740\n",
            "Epoch 98/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7394 - acc: 0.7764 - val_loss: 0.6922 - val_acc: 0.7978\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.80740\n",
            "Epoch 99/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.7312 - acc: 0.7768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 38s 192ms/step - loss: 0.7324 - acc: 0.7775 - val_loss: 0.6556 - val_acc: 0.8067\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.80740\n",
            "Epoch 100/200\n",
            "196/196 [==============================] - 36s 185ms/step - loss: 0.7362 - acc: 0.7770 - val_loss: 0.6718 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.80740\n",
            "Epoch 101/200\n",
            " 82/196 [===========>..................] - ETA: 19s - loss: 0.7404 - acc: 0.7756"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7370 - acc: 0.7761 - val_loss: 0.6703 - val_acc: 0.8042\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.80740\n",
            "Epoch 102/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7357 - acc: 0.7770 - val_loss: 0.6673 - val_acc: 0.8048\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.80740\n",
            "Epoch 103/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7317 - acc: 0.7783"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7337 - acc: 0.7777 - val_loss: 0.6659 - val_acc: 0.8038\n",
            "\n",
            "Epoch 00103: val_acc did not improve from 0.80740\n",
            "Epoch 104/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7295 - acc: 0.7778 - val_loss: 0.6725 - val_acc: 0.8031\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.80740\n",
            "Epoch 105/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7332 - acc: 0.7762"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7298 - acc: 0.7770 - val_loss: 0.6580 - val_acc: 0.8066\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.80740\n",
            "Epoch 106/200\n",
            "196/196 [==============================] - 36s 186ms/step - loss: 0.7238 - acc: 0.7813 - val_loss: 0.6591 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.80740\n",
            "Epoch 107/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7291 - acc: 0.7813"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7266 - acc: 0.7795 - val_loss: 0.6641 - val_acc: 0.8044\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.80740\n",
            "Epoch 108/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7240 - acc: 0.7800 - val_loss: 0.6629 - val_acc: 0.8057\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.80740\n",
            "Epoch 109/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.7267 - acc: 0.7778"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7264 - acc: 0.7795 - val_loss: 0.6476 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00109: val_acc improved from 0.80740 to 0.80850, saving model to /content/saved_models/convnet_agmentation_20_model.109.h5\n",
            "Epoch 110/200\n",
            "196/196 [==============================] - 37s 186ms/step - loss: 0.7215 - acc: 0.7797 - val_loss: 0.6873 - val_acc: 0.8002\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.80850\n",
            "Epoch 111/200\n",
            " 39/196 [====>.........................] - ETA: 25s - loss: 0.7299 - acc: 0.7771"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7239 - acc: 0.7811 - val_loss: 0.6736 - val_acc: 0.8004\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.80850\n",
            "Epoch 112/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7189 - acc: 0.7835 - val_loss: 0.6509 - val_acc: 0.8066\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.80850\n",
            "Epoch 113/200\n",
            " 71/196 [=========>....................] - ETA: 21s - loss: 0.7268 - acc: 0.7771"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7231 - acc: 0.7801 - val_loss: 0.6620 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.80850\n",
            "Epoch 114/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7242 - acc: 0.7790 - val_loss: 0.6561 - val_acc: 0.8071\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.80850\n",
            "Epoch 115/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7129 - acc: 0.7812"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7190 - acc: 0.7806 - val_loss: 0.6516 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.80850\n",
            "Epoch 116/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7210 - acc: 0.7791 - val_loss: 0.6721 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.80850\n",
            "Epoch 117/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.7056 - acc: 0.7852"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7148 - acc: 0.7811 - val_loss: 0.6219 - val_acc: 0.8163\n",
            "\n",
            "Epoch 00117: val_acc improved from 0.80850 to 0.81630, saving model to /content/saved_models/convnet_agmentation_20_model.117.h5\n",
            "Epoch 118/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7150 - acc: 0.7826 - val_loss: 0.6480 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.81630\n",
            "Epoch 119/200\n",
            " 38/196 [====>.........................] - ETA: 25s - loss: 0.7219 - acc: 0.7835"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7204 - acc: 0.7788 - val_loss: 0.6559 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.81630\n",
            "Epoch 120/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7171 - acc: 0.7827 - val_loss: 0.6575 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.81630\n",
            "Epoch 121/200\n",
            " 72/196 [==========>...................] - ETA: 21s - loss: 0.7095 - acc: 0.7843"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7095 - acc: 0.7825 - val_loss: 0.6628 - val_acc: 0.8049\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.81630\n",
            "Epoch 122/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7124 - acc: 0.7827 - val_loss: 0.6465 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.81630\n",
            "Epoch 123/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7022 - acc: 0.7840"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.7050 - acc: 0.7841 - val_loss: 0.6438 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.81630\n",
            "Epoch 124/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7054 - acc: 0.7861 - val_loss: 0.6447 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.81630\n",
            "Epoch 125/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.7025 - acc: 0.7862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7092 - acc: 0.7847 - val_loss: 0.6474 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.81630\n",
            "Epoch 126/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7096 - acc: 0.7839 - val_loss: 0.6475 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.81630\n",
            "Epoch 127/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7167 - acc: 0.7824"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7094 - acc: 0.7842 - val_loss: 0.6409 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.81630\n",
            "Epoch 128/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7089 - acc: 0.7839 - val_loss: 0.6441 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.81630\n",
            "Epoch 129/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7000 - acc: 0.7858"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7035 - acc: 0.7862 - val_loss: 0.6419 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.81630\n",
            "Epoch 130/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7099 - acc: 0.7826 - val_loss: 0.6449 - val_acc: 0.8075\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.81630\n",
            "Epoch 131/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.6984 - acc: 0.7861"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7084 - acc: 0.7837 - val_loss: 0.6434 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00131: val_acc did not improve from 0.81630\n",
            "Epoch 132/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7037 - acc: 0.7847 - val_loss: 0.6436 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.81630\n",
            "Epoch 133/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7138 - acc: 0.7818"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7080 - acc: 0.7838 - val_loss: 0.6441 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.81630\n",
            "Epoch 134/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7075 - acc: 0.7823 - val_loss: 0.6442 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.81630\n",
            "Epoch 135/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7090 - acc: 0.7852"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.7075 - acc: 0.7841 - val_loss: 0.6486 - val_acc: 0.8079\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.81630\n",
            "Epoch 136/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7017 - acc: 0.7867 - val_loss: 0.6489 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.81630\n",
            "Epoch 137/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7029 - acc: 0.7847"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7062 - acc: 0.7836 - val_loss: 0.6449 - val_acc: 0.8079\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.81630\n",
            "Epoch 138/200\n",
            "196/196 [==============================] - 36s 186ms/step - loss: 0.7039 - acc: 0.7843 - val_loss: 0.6434 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.81630\n",
            "Epoch 139/200\n",
            " 82/196 [===========>..................] - ETA: 19s - loss: 0.7024 - acc: 0.7851"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 36s 186ms/step - loss: 0.7048 - acc: 0.7838 - val_loss: 0.6452 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.81630\n",
            "Epoch 140/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7069 - acc: 0.7859 - val_loss: 0.6461 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.81630\n",
            "Epoch 141/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7086 - acc: 0.7854"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7077 - acc: 0.7835 - val_loss: 0.6416 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.81630\n",
            "Epoch 142/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7038 - acc: 0.7841 - val_loss: 0.6428 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.81630\n",
            "Epoch 143/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7034 - acc: 0.7841"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7050 - acc: 0.7847 - val_loss: 0.6406 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.81630\n",
            "Epoch 144/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7033 - acc: 0.7840 - val_loss: 0.6450 - val_acc: 0.8089\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.81630\n",
            "Epoch 145/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7082 - acc: 0.7853"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7023 - acc: 0.7871 - val_loss: 0.6474 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.81630\n",
            "Epoch 146/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7043 - acc: 0.7846 - val_loss: 0.6499 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.81630\n",
            "Epoch 147/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.6921 - acc: 0.7896"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7040 - acc: 0.7852 - val_loss: 0.6458 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.81630\n",
            "Epoch 148/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7004 - acc: 0.7867 - val_loss: 0.6455 - val_acc: 0.8083\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.81630\n",
            "Epoch 149/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7002 - acc: 0.7881"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7045 - acc: 0.7844 - val_loss: 0.6415 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.81630\n",
            "Epoch 150/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7033 - acc: 0.7873 - val_loss: 0.6423 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.81630\n",
            "Epoch 151/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.6986 - acc: 0.7847"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.7025 - acc: 0.7852 - val_loss: 0.6454 - val_acc: 0.8087\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.81630\n",
            "Epoch 152/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7019 - acc: 0.7853 - val_loss: 0.6452 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.81630\n",
            "Epoch 153/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7081 - acc: 0.7868"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7071 - acc: 0.7864 - val_loss: 0.6434 - val_acc: 0.8085\n",
            "\n",
            "Epoch 00153: val_acc did not improve from 0.81630\n",
            "Epoch 154/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7018 - acc: 0.7869 - val_loss: 0.6415 - val_acc: 0.8093\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.81630\n",
            "Epoch 155/200\n",
            " 81/196 [===========>..................] - ETA: 19s - loss: 0.7004 - acc: 0.7878"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7068 - acc: 0.7841 - val_loss: 0.6439 - val_acc: 0.8088\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.81630\n",
            "Epoch 156/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6965 - acc: 0.7884 - val_loss: 0.6420 - val_acc: 0.8090\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.81630\n",
            "Epoch 157/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.7022 - acc: 0.7823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7033 - acc: 0.7845 - val_loss: 0.6423 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.81630\n",
            "Epoch 158/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7032 - acc: 0.7858 - val_loss: 0.6430 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.81630\n",
            "Epoch 159/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.6974 - acc: 0.7883"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7017 - acc: 0.7853 - val_loss: 0.6389 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.81630\n",
            "Epoch 160/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7022 - acc: 0.7852 - val_loss: 0.6399 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.81630\n",
            "Epoch 161/200\n",
            " 80/196 [===========>..................] - ETA: 19s - loss: 0.6993 - acc: 0.7872"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7020 - acc: 0.7853 - val_loss: 0.6370 - val_acc: 0.8105\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.81630\n",
            "Epoch 162/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7041 - acc: 0.7839 - val_loss: 0.6375 - val_acc: 0.8104\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.81630\n",
            "Epoch 163/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7037 - acc: 0.7850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7042 - acc: 0.7843 - val_loss: 0.6381 - val_acc: 0.8106\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.81630\n",
            "Epoch 164/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7072 - acc: 0.7817 - val_loss: 0.6391 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00164: val_acc did not improve from 0.81630\n",
            "Epoch 165/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7000 - acc: 0.7880"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 186ms/step - loss: 0.7017 - acc: 0.7857 - val_loss: 0.6396 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.81630\n",
            "Epoch 166/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7033 - acc: 0.7853 - val_loss: 0.6393 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.81630\n",
            "Epoch 167/200\n",
            " 76/196 [==========>...................] - ETA: 20s - loss: 0.7034 - acc: 0.7859"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7010 - acc: 0.7858 - val_loss: 0.6391 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.81630\n",
            "Epoch 168/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6994 - acc: 0.7837 - val_loss: 0.6396 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.81630\n",
            "Epoch 169/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.6888 - acc: 0.7893"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7025 - acc: 0.7860 - val_loss: 0.6395 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.81630\n",
            "Epoch 170/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7026 - acc: 0.7840 - val_loss: 0.6393 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00170: val_acc did not improve from 0.81630\n",
            "Epoch 171/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7015 - acc: 0.7831"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7058 - acc: 0.7834 - val_loss: 0.6390 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.81630\n",
            "Epoch 172/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7069 - acc: 0.7832 - val_loss: 0.6388 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00172: val_acc did not improve from 0.81630\n",
            "Epoch 173/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.6955 - acc: 0.7900"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7010 - acc: 0.7865 - val_loss: 0.6395 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.81630\n",
            "Epoch 174/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7002 - acc: 0.7852 - val_loss: 0.6394 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.81630\n",
            "Epoch 175/200\n",
            " 81/196 [===========>..................] - ETA: 20s - loss: 0.7012 - acc: 0.7836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7002 - acc: 0.7868 - val_loss: 0.6390 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.81630\n",
            "Epoch 176/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7001 - acc: 0.7872 - val_loss: 0.6392 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.81630\n",
            "Epoch 177/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.6979 - acc: 0.7875"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7000 - acc: 0.7888 - val_loss: 0.6393 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.81630\n",
            "Epoch 178/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7016 - acc: 0.7867 - val_loss: 0.6392 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.81630\n",
            "Epoch 179/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.7053 - acc: 0.7840"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7032 - acc: 0.7839 - val_loss: 0.6403 - val_acc: 0.8102\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.81630\n",
            "Epoch 180/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7006 - acc: 0.7856 - val_loss: 0.6400 - val_acc: 0.8103\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.81630\n",
            "Epoch 181/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7020 - acc: 0.7862"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6986 - acc: 0.7857 - val_loss: 0.6400 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.81630\n",
            "Epoch 182/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6972 - acc: 0.7854 - val_loss: 0.6401 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.81630\n",
            "Epoch 183/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7125 - acc: 0.7823"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7034 - acc: 0.7861 - val_loss: 0.6397 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.81630\n",
            "Epoch 184/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7005 - acc: 0.7854 - val_loss: 0.6396 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.81630\n",
            "Epoch 185/200\n",
            " 76/196 [==========>...................] - ETA: 20s - loss: 0.7056 - acc: 0.7858"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6984 - acc: 0.7879 - val_loss: 0.6397 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.81630\n",
            "Epoch 186/200\n",
            "196/196 [==============================] - 37s 189ms/step - loss: 0.7015 - acc: 0.7863 - val_loss: 0.6395 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.81630\n",
            "Epoch 187/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.7024 - acc: 0.7843"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7022 - acc: 0.7855 - val_loss: 0.6401 - val_acc: 0.8097\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.81630\n",
            "Epoch 188/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7066 - acc: 0.7831 - val_loss: 0.6401 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.81630\n",
            "Epoch 189/200\n",
            " 77/196 [==========>...................] - ETA: 20s - loss: 0.6946 - acc: 0.7849"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7020 - acc: 0.7841 - val_loss: 0.6404 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.81630\n",
            "Epoch 190/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6951 - acc: 0.7879 - val_loss: 0.6404 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.81630\n",
            "Epoch 191/200\n",
            " 78/196 [==========>...................] - ETA: 20s - loss: 0.6980 - acc: 0.7871"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.6990 - acc: 0.7864 - val_loss: 0.6403 - val_acc: 0.8101\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.81630\n",
            "Epoch 192/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7032 - acc: 0.7883 - val_loss: 0.6397 - val_acc: 0.8099\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.81630\n",
            "Epoch 193/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.6957 - acc: 0.7874"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.6972 - acc: 0.7860 - val_loss: 0.6393 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.81630\n",
            "Epoch 194/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7020 - acc: 0.7849 - val_loss: 0.6410 - val_acc: 0.8096\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.81630\n",
            "Epoch 195/200\n",
            " 81/196 [===========>..................] - ETA: 19s - loss: 0.6923 - acc: 0.7893"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.6983 - acc: 0.7868 - val_loss: 0.6402 - val_acc: 0.8097\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.81630\n",
            "Epoch 196/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7049 - acc: 0.7853 - val_loss: 0.6410 - val_acc: 0.8094\n",
            "\n",
            "Epoch 00196: val_acc did not improve from 0.81630\n",
            "Epoch 197/200\n",
            " 79/196 [===========>..................] - ETA: 20s - loss: 0.7040 - acc: 0.7837"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 188ms/step - loss: 0.6978 - acc: 0.7856 - val_loss: 0.6397 - val_acc: 0.8098\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.81630\n",
            "Epoch 198/200\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.7008 - acc: 0.7860 - val_loss: 0.6407 - val_acc: 0.8095\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.81630\n",
            "Epoch 199/200\n",
            " 80/196 [===========>..................] - ETA: 20s - loss: 0.7072 - acc: 0.7825"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7020 - acc: 0.7850 - val_loss: 0.6416 - val_acc: 0.8096\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.81630\n",
            "Epoch 200/200\n",
            "196/196 [==============================] - 37s 187ms/step - loss: 0.7033 - acc: 0.7867 - val_loss: 0.6408 - val_acc: 0.8096\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.81630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uw6mKlOLHqor",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Observation and Discussion \n",
        "\n",
        "The data augmentation step did improve the results by 2 or 3 % rising from 78% as previously shown in the simple architecture to 81.6%. It is worth to mention that this is the only model that is underfited as you see the testing accuracy is higher than the training accuracy in almost all the epoches. The explanation to this can be either the dropouts are fairly aggresive. Another explanation is that the testing data is not noisy as the training data so that causes the model to be ambigious classifying the noisy set and deeper layers is required to learn these noisy examples. Due to time limitation, I will leave this approach here for now and I wont investigate more to find out why. "
      ]
    },
    {
      "metadata": {
        "id": "dMe4d3d4mXyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Xception Model: \n",
        "\n",
        "We can think of the Xception model as an extreme way of doing inception. It is based on the hypothesis that cross-channel correlations and spatial correlations are sufficiently decoupled from each other and it is preferable not to map them jointly. The regular convolution layer basically layer basiacly seeks out both correlations in one go which is not prefared by this hypothesis. The regular convolution filter simultaneously considers a spatial dimension and a cross-channel dimension (depth of the image RBG). In [5] the author basically argues  we do we need to consider both the image region and the channels (RBG) at the same time ? The Inception model uses 11 convolution to project the original input into separate chunks of input spaces. Then different types of filters are applied to each space to transform these chunks. Unlike Xception model, instead of partitioning input data into several compressed chunks, it maps the spatial correlations for each output channel separately, and then performs a 11 depthwise convolution to capture cross-channel correlation. The author notes that this is essentially equivalent to an existing operation known as a depthwise separable convolution\", which is also an efficient way to estimate convolution. In [5] the author shows that the Xception model outperforms Inception v3 on the ImageNet dataset, and vastly outperforms it on a larger image classification dataset with 17,000 classes. \n",
        "\n",
        "The Xception model is computationaly very efficient as shown in [5]. Based on my own humble observation it has about 20 million trainable parameters which is dramatically more parameters than a Resnet 56 but the average epoch time takes slighly more time than the Xception module. Xception is much newer (it came out in April 2017), but its architecture is already powering Googles mobile vision applications through MobileNet."
      ]
    },
    {
      "metadata": {
        "id": "3hcTJUybHeiw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "hLZfuBH2Iv43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4896
        },
        "outputId": "67432404-d519-41a8-b5bc-764858e3e9be"
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.xception import Xception\n",
        "\n",
        "# retrain model or use saved model\n",
        "retrain_xception = True\n",
        "#save model_after_train \n",
        "model_name_xception = '/xception_trained.h5'\n",
        "# Data augmentation \n",
        "data_augmentation = True\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "input_tensor = Input(shape=x_train.shape[1:])\n",
        "model = Xception(weights=None,input_tensor=input_tensor,classes=10)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=learning_rate_adjuster(0)),metrics=['accuracy'])\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "m_name3 = 'xception_%d_model.{epoch:03d}.h5' % depth\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, m_name3)\n",
        "# callbacks for best saved models and for learning rates\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_acc',verbose=1,save_best_only=True)\n",
        "lr_scheduler = LearningRateScheduler(learning_rate_adjuster)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),cooldown=0,patience=5,min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "model.summary()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 15, 15, 32)   864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 15, 15, 32)   128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 15, 15, 32)   0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 13, 13, 64)   18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 13, 13, 64)   256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 13, 13, 64)   0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 13, 13, 128)  8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 13, 13, 128)  512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 13, 13, 128)  0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 13, 13, 128)  17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 13, 13, 128)  512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 7, 7, 128)    8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 7, 7, 128)    0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 7, 7, 128)    512         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 7, 7, 128)    0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 7, 7, 128)    0           add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 7, 7, 256)    33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 7, 7, 256)    1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 7, 7, 256)    0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 7, 7, 256)    67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 7, 7, 256)    1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 4, 4, 256)    32768       add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 4, 4, 256)    0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 4, 4, 256)    1024        conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 4, 4, 256)    0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 4, 4, 256)    0           add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 4, 4, 728)    188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 4, 4, 728)    2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 4, 4, 728)    0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 4, 4, 728)    536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 4, 4, 728)    2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 2, 2, 728)    186368      add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 2, 2, 728)    0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 2, 2, 728)    2912        conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 2, 2, 728)    0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 2, 2, 728)    0           add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 2, 2, 728)    536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 2, 2, 728)    2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 2, 2, 728)    0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 2, 2, 728)    536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 2, 2, 728)    2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 2, 2, 728)    0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 2, 2, 728)    536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 2, 2, 728)    2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 2, 2, 728)    0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 2, 2, 728)    0           add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 2, 2, 728)    536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 2, 2, 728)    2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 2, 2, 728)    0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 2, 2, 728)    536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 2, 2, 728)    2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 2, 2, 728)    0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 2, 2, 728)    536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 2, 2, 728)    2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 2, 2, 728)    0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 2, 2, 728)    0           add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 2, 2, 728)    536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 2, 2, 728)    2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 2, 2, 728)    0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 2, 2, 728)    536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 2, 2, 728)    2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 2, 2, 728)    0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 2, 2, 728)    536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 2, 2, 728)    2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 2, 2, 728)    0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 2, 2, 728)    0           add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 2, 2, 728)    536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 2, 2, 728)    2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 2, 2, 728)    0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 2, 2, 728)    536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 2, 2, 728)    2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 2, 2, 728)    0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 2, 2, 728)    536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 2, 2, 728)    2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 2, 2, 728)    0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 2, 2, 728)    0           add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 2, 2, 728)    536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 2, 2, 728)    2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 2, 2, 728)    0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 2, 2, 728)    536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 2, 2, 728)    2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 2, 2, 728)    0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 2, 2, 728)    536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 2, 2, 728)    2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 2, 2, 728)    0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 2, 2, 728)    0           add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 2, 2, 728)    536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 2, 2, 728)    2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 2, 2, 728)    0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 2, 2, 728)    536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 2, 2, 728)    2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 2, 2, 728)    0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 2, 2, 728)    536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 2, 2, 728)    2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 2, 2, 728)    0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 2, 2, 728)    0           add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 2, 2, 728)    536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 2, 2, 728)    2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 2, 2, 728)    0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 2, 2, 728)    536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 2, 2, 728)    2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 2, 2, 728)    0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 2, 2, 728)    536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 2, 2, 728)    2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 2, 2, 728)    0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 2, 2, 728)    0           add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 2, 2, 728)    536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 2, 2, 728)    2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 2, 2, 728)    0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 2, 2, 728)    536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 2, 2, 728)    2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 2, 2, 728)    0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 2, 2, 728)    536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 2, 2, 728)    2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 2, 2, 728)    0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 2, 2, 728)    0           add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 2, 2, 728)    536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 2, 2, 728)    2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 2, 2, 728)    0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 2, 2, 1024)   752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 2, 2, 1024)   4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 1, 1, 1024)   745472      add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 1, 1, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 1, 1, 1024)   4096        conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 1, 1, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 1, 1, 1536)   1582080     add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 1, 1, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 1, 1, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 1, 1, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 1, 1, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 1, 1, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           20490       avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 20,881,970\n",
            "Trainable params: 20,827,442\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QiIXQUP-KpJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2774
        },
        "outputId": "de17f8dd-c5c5-40d3-d8c1-3d08c17aa156"
      },
      "cell_type": "code",
      "source": [
        "print (save_dir)\n",
        "if retrain_xception: \n",
        "  history = model.fit_generator(datagenerator.flow(x_train, y_train, batch_size=batch_size),\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            epochs=40, verbose=1, workers=4,\n",
        "                            callbacks=callbacks)\n",
        "  model = history.model\n",
        "  model.save_weights(save_dir+model_name_xception)\n",
        "else:\n",
        "  model.load_weights(save_dir+model_name_xception)\n",
        "  print ('Model weights loaded')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/saved_models\n",
            "Epoch 1/40\n",
            "391/391 [==============================] - 108s 276ms/step - loss: 0.1024 - acc: 0.9642 - val_loss: 0.6369 - val_acc: 0.8456\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.87750\n",
            "Epoch 2/40\n",
            "184/391 [=============>................] - ETA: 58s - loss: 0.0911 - acc: 0.9677"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 116s 296ms/step - loss: 0.0980 - acc: 0.9645 - val_loss: 0.6243 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.87750\n",
            "Epoch 3/40\n",
            "310/391 [======================>.......] - ETA: 21s - loss: 0.0982 - acc: 0.9649"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 109s 278ms/step - loss: 0.0967 - acc: 0.9656 - val_loss: 0.5641 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.87750\n",
            "Epoch 4/40\n",
            "348/391 [=========================>....] - ETA: 11s - loss: 0.0884 - acc: 0.9689"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 271ms/step - loss: 0.0910 - acc: 0.9681 - val_loss: 0.6388 - val_acc: 0.8493\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.87750\n",
            "Epoch 5/40\n",
            "360/391 [==========================>...] - ETA: 8s - loss: 0.0862 - acc: 0.9694"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 269ms/step - loss: 0.0863 - acc: 0.9695 - val_loss: 0.6171 - val_acc: 0.8495\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.87750\n",
            "Epoch 6/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0892 - acc: 0.9685"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 104s 267ms/step - loss: 0.0895 - acc: 0.9683 - val_loss: 0.6184 - val_acc: 0.8529\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87750\n",
            "Epoch 7/40\n",
            "364/391 [==========================>...] - ETA: 7s - loss: 0.0862 - acc: 0.9700"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 112s 286ms/step - loss: 0.0868 - acc: 0.9695 - val_loss: 0.5682 - val_acc: 0.8601\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87750\n",
            "Epoch 8/40\n",
            "364/391 [==========================>...] - ETA: 7s - loss: 0.0828 - acc: 0.9703"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 112s 286ms/step - loss: 0.0832 - acc: 0.9702 - val_loss: 0.6459 - val_acc: 0.8482\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87750\n",
            "Epoch 9/40\n",
            "364/391 [==========================>...] - ETA: 7s - loss: 0.0826 - acc: 0.9704"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 272ms/step - loss: 0.0825 - acc: 0.9705 - val_loss: 0.6318 - val_acc: 0.8525\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.87750\n",
            "Epoch 10/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0819 - acc: 0.9708"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 269ms/step - loss: 0.0825 - acc: 0.9706 - val_loss: 0.6478 - val_acc: 0.8536\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87750\n",
            "Epoch 11/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0838 - acc: 0.9706"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 114s 291ms/step - loss: 0.0833 - acc: 0.9707 - val_loss: 0.6293 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.87750\n",
            "Epoch 12/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0839 - acc: 0.9702"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 113s 289ms/step - loss: 0.0837 - acc: 0.9703 - val_loss: 0.6193 - val_acc: 0.8486\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.87750\n",
            "Epoch 13/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0790 - acc: 0.9714"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 268ms/step - loss: 0.0796 - acc: 0.9716 - val_loss: 0.5893 - val_acc: 0.8569\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.87750\n",
            "Epoch 14/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0778 - acc: 0.9736"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 270ms/step - loss: 0.0784 - acc: 0.9733 - val_loss: 0.5877 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.87750\n",
            "Epoch 15/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0780 - acc: 0.9725"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 267ms/step - loss: 0.0785 - acc: 0.9725 - val_loss: 0.6008 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.87750\n",
            "Epoch 16/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0732 - acc: 0.9738"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 269ms/step - loss: 0.0745 - acc: 0.9733 - val_loss: 0.6433 - val_acc: 0.8528\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.87750\n",
            "Epoch 17/40\n",
            "363/391 [==========================>...] - ETA: 8s - loss: 0.0731 - acc: 0.9749"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 117s 300ms/step - loss: 0.0740 - acc: 0.9743 - val_loss: 0.6118 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.87750\n",
            "Epoch 18/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0754 - acc: 0.9743"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 108s 276ms/step - loss: 0.0757 - acc: 0.9742 - val_loss: 0.6092 - val_acc: 0.8585\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.87750\n",
            "Epoch 19/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0737 - acc: 0.9743"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 271ms/step - loss: 0.0744 - acc: 0.9740 - val_loss: 0.6345 - val_acc: 0.8480\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.87750\n",
            "Epoch 20/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0732 - acc: 0.9732"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 111s 285ms/step - loss: 0.0734 - acc: 0.9731 - val_loss: 0.6252 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.87750\n",
            "Epoch 21/40\n",
            "363/391 [==========================>...] - ETA: 8s - loss: 0.0724 - acc: 0.9745"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 116s 297ms/step - loss: 0.0734 - acc: 0.9743 - val_loss: 0.6019 - val_acc: 0.8549\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.87750\n",
            "Epoch 22/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0725 - acc: 0.9742"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 270ms/step - loss: 0.0730 - acc: 0.9742 - val_loss: 0.5884 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.87750\n",
            "Epoch 23/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0742 - acc: 0.9744"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 271ms/step - loss: 0.0735 - acc: 0.9745 - val_loss: 0.6452 - val_acc: 0.8553\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.87750\n",
            "Epoch 24/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0753 - acc: 0.9733"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 269ms/step - loss: 0.0757 - acc: 0.9734 - val_loss: 0.6174 - val_acc: 0.8579\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.87750\n",
            "Epoch 25/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0728 - acc: 0.9749"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 268ms/step - loss: 0.0729 - acc: 0.9749 - val_loss: 0.6400 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.87750\n",
            "Epoch 26/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0692 - acc: 0.9754"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 115s 294ms/step - loss: 0.0699 - acc: 0.9751 - val_loss: 0.6367 - val_acc: 0.8570\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.87750\n",
            "Epoch 27/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0713 - acc: 0.9739"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 110s 281ms/step - loss: 0.0712 - acc: 0.9738 - val_loss: 0.6210 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.87750\n",
            "Epoch 28/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0697 - acc: 0.9749"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 104s 267ms/step - loss: 0.0700 - acc: 0.9747 - val_loss: 0.5913 - val_acc: 0.8609\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.87750\n",
            "Epoch 29/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0663 - acc: 0.9770"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 112s 286ms/step - loss: 0.0672 - acc: 0.9765 - val_loss: 0.6102 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.87750\n",
            "Epoch 30/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0686 - acc: 0.9761"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 117s 299ms/step - loss: 0.0688 - acc: 0.9760 - val_loss: 0.6215 - val_acc: 0.8593\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.87750\n",
            "Epoch 31/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0687 - acc: 0.9762"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 108s 275ms/step - loss: 0.0684 - acc: 0.9762 - val_loss: 0.6409 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.87750\n",
            "Epoch 32/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0637 - acc: 0.9780"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 106s 272ms/step - loss: 0.0634 - acc: 0.9779 - val_loss: 0.6130 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.87750\n",
            "Epoch 33/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0638 - acc: 0.9788"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 268ms/step - loss: 0.0651 - acc: 0.9783 - val_loss: 0.6408 - val_acc: 0.8539\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.87750\n",
            "Epoch 34/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0634 - acc: 0.9784"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 270ms/step - loss: 0.0635 - acc: 0.9784 - val_loss: 0.6305 - val_acc: 0.8568\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.87750\n",
            "Epoch 35/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0661 - acc: 0.9765"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 107s 275ms/step - loss: 0.0662 - acc: 0.9764 - val_loss: 0.6063 - val_acc: 0.8590\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.87750\n",
            "Epoch 36/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0647 - acc: 0.9769"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 113s 290ms/step - loss: 0.0648 - acc: 0.9769 - val_loss: 0.6097 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.87750\n",
            "Epoch 37/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0650 - acc: 0.9766"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 107s 273ms/step - loss: 0.0645 - acc: 0.9768 - val_loss: 0.6498 - val_acc: 0.8504\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.87750\n",
            "Epoch 38/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0631 - acc: 0.9780"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 105s 268ms/step - loss: 0.0629 - acc: 0.9782 - val_loss: 0.6101 - val_acc: 0.8607\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.87750\n",
            "Epoch 39/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0618 - acc: 0.9780"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 116s 297ms/step - loss: 0.0619 - acc: 0.9780 - val_loss: 0.6013 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.87750\n",
            "Epoch 40/40\n",
            "363/391 [==========================>...] - ETA: 7s - loss: 0.0631 - acc: 0.9782"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 112s 286ms/step - loss: 0.0635 - acc: 0.9781 - val_loss: 0.6561 - val_acc: 0.8513\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.87750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UYqYMSJiHkP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Observation and Discussion \n",
        "\n",
        "The model scored slighly above 87% and again like the other deep models it tends to overfit at higher epoches levels while attempting to increase the accuracy of the overall model this problem should be addressed. It is worth to mention that the number of parameters of this model is more than 20 million trainable parameters, however, the average epoch time is only about104 seconds. On the other hand the ResnetV2 of 20 layers has about 500K trainable parameters and it took 59 seconds on average for each epoch. The aveage epoch time for a deeper Resnet can surpase the amount of time required to train it than that of an Xception model. This statement doesnt imply that the Xception model is generaly better or more efficient than the Resnet20, but it implies that when depthwise separable convolution is used it can generaly make the models more computationaly efficient especially for a deep model like an xception model. "
      ]
    },
    {
      "metadata": {
        "id": "5WSOGVV2mm3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#CONCLUSION AND FUTURE DIRECTION: \n",
        "\n",
        "A resnetv2 of depth equal to 20 is implemented which achieved almost 91% accuracy. At first a simple convolution architecture can achieve a reasonable good accuracy of almost 82% accuracy and a deeper architecture shows only 9% more accuracy. In [1] they show that a Resnet of depth equal to 1001 can achieve almost 95% accuracy but the computational complexity is fairly large. For this reason I didnot pursue this experiement due to time limitation and it is only 3% to 4% extra gains, also I thought it would be more interesteing to try other approaches. Data augmentation and learning rate scheduling improved the simple convolution net by 2%. The Xception model achieved more than 87% accuracy given that the complexity of this model is fairly less than a Resnet archietcture. More than 20 million parameters were learned in a dramaitically shorter time if you compare the same condition with a very deep architecture like Resent1001. The concept of depthwise separable convolution shown to be computationaly efficient. I ran the models on 200 epoches but it looks like it doesnt need that much computational time as the models stoped improving so I reduced it to 100 with minimal tradeoff on accuracy outcome. \n",
        "\n",
        "Overall the deep architectures tends to over fit as you can see in the implementations the training accuracy is alwasy higher than the testing accuracy by 9%. This was not the case with the simple convolution architecture provided that dropouts and regularizations was used. Even though batch normalizations were used in these deep architectures but it may not be sufficient for this dataset and it might be a good idea to try dropouts and/or L2 regularization. \n",
        "\n",
        "Still arguably and undeniable that a deeper architecture can lead to the best results, however, it may not be very practical because of its training time. This issue can be addressed by using more efficient ways to compute convolution and conduct a research in this direction. There are also other techniques such as transfer learning and synthetic gradients. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "n5qnqPppeO37",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tranfer Learning and Synthetic Gradients: \n",
        "\n",
        "A quick experiement that I wanted to do is to basically use an already pretrained Resnet1001 on the image net data set. Then we can freeze the first layers and mid layers and retrain the last few layers on the new data set. This is usually refered to as trasfer learning in the litreture. This can save some training time. \n",
        "\n",
        "Personaly, I think machine vision is still far away from human performance in terms of image classificaiton. Humans are far more robust and efficient in image classificaiton outperforming Machine Learning in noisy enviroments. May be our visual system can be compared with a fairly very deep neural network, but our visual system is trained over the years, and we are trying to train a new deep network within hours. So from here I think it makes sense to research better ways to train these networks rather than relying on backprogation. The concept of Synthetic gradients introduced in [6] has shown some efficient convergence techninques and it is also used by google's DeepMind. The idea of Synthetic gradients is to be able to update parts of a neural network asynchronously or executed through parallel computing without waiting for gradient update of the previous layers like backpropagation. This allows for faster training of deep and complex architectures such as a ResnetV2 1001. "
      ]
    },
    {
      "metadata": {
        "id": "Pf2Q3dIzuEy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ACKNOWLEDGMENT\n",
        "\n",
        "Regardless of the outcome of this interview assesment I would like to thank you for this oppurtuenty, it made me revamp my machine vision concepts and I had fun working on this assesment. I wish I had more time to do more experiments that I had in mind but I was not able to commit more than 2 hours a day on average (excluding code execution time) the past week because of other work commitments I had on my end. "
      ]
    },
    {
      "metadata": {
        "id": "zoAcoMOiupY8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# REFRENCES\n",
        "\n",
        "[1] Sun, et al. 'Identity Mappings in Deep Residual Networks', [LINK](https://arxiv.org/pdf/1603.05027.pdf)\n",
        "\n",
        "[2] Alemi, et al. 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning', [LINK](https://arxiv.org/pdf/1602.07261.pdf)\n",
        "\n",
        "[3] Sun, et al. 'Deep Residual Learning for Image Recognition' [LINK](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "[4] Louis Prove. 'An Introduction to different Types of Convolutions in Deep Learning' [LINK](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)\n",
        "\n",
        "[5] Chollet. 'Xception: Deep Learning with Depthwise Separable Convolutions' [LINK](https://arxiv.org/abs/1610.02357)\n",
        "\n",
        "[6] Kavukcuoglu, et al. 'Decoupled Neural Interfaces using Synthetic Gradients' [LINK](https://arxiv.org/pdf/1608.05343.pdf)"
      ]
    }
  ]
}